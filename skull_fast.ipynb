{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import math\nimport os\nfrom itertools import islice\nsep = os.sep\nimport numpy as np\nimport pydicom\nfrom torch.utils.data.dataloader import DataLoader\nfrom torch.utils.data.dataset import Dataset, random_split\nimport torchvision.transforms as tmf\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import img_utils as iu\nimport nnviz as viz\nfrom measurements import ScoreAccumulator","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EPIDURAL = 'epidural'\nINTRAPARENCHYMAL = 'intraparenchymal'\nINTRAVENTRICULAR = 'intraventricular'\nSUBARACHNOID = 'subarachnoid'\nSUBDURAL = 'subdural'\nANY = 'any'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_mapping_file = '../input/rsna-intracranial-hemorrhage-detection/stage_1_train.csv'\ntrain_images_dir = '../input/rsna-intracranial-hemorrhage-detection/stage_1_train_images/'\ntest_mapping_file = '../input/rsna-intracranial-hemorrhage-detection/stage_1_sample_submission.csv'\ntest_images_dir = '../input/rsna-intracranial-hemorrhage-detection/stage_1_test_images/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_transforms = tmf.Compose([\n    tmf.ToPILImage(),\n    tmf.Resize((512, 512), interpolation=2),\n    tmf.RandomHorizontalFlip(),\n    tmf.RandomVerticalFlip(),\n    tmf.ToTensor()\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_transforms = tmf.Compose([\n    tmf.ToPILImage(),\n    tmf.Resize((512, 512), interpolation=2),\n    tmf.ToTensor()\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.utils.data._utils.collate import default_collate\ndef clean_collate(batch):\n    return default_collate([b for b in batch if b])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class SkullDataset(Dataset):\n    def __init__(self, conf=None, mode=None, transforms=None):\n        self.transforms = transforms\n        self.mode = mode\n        self.conf = conf\n        self.image_dir = None\n        self.mapping_file = None\n        self.expand_by = self.conf.get('Params').get('expand_patch_by')\n        self.indices = []\n        self.LIM = conf.get('load_lim', 10e10)\n\n    def load_data_indices(self, validate_pth=False):\n        with open(self.mapping_file) as infile:\n            linecount, six_rows, _ = 1, True, next(infile)\n            while six_rows:\n                print('Reading Line: {}'.format(linecount), end='\\r')\n\n                six_rows = list(r.rstrip().split(',') for r in islice(infile, 6))\n                image_file, cat_label = None, []\n                for hname, label in six_rows:\n                    (ID, file_ID, htype), label = hname.split('_'), int(label)\n                    fname_ = ID + '_' + file_ID + '.dcm'\n\n                    if validate_pth and not os.path.exists(os.path.join(self.image_dir, fname_)):\n                        break\n\n                    if image_file and fname_ != image_file:\n                        print('Mismatch Line: {}'.format(linecount), end='\\r')\n                        break\n                    else:\n                        image_file = fname_\n\n                    cat_label.append(label)\n\n                if image_file and len(cat_label) == 6:\n                    self.indices.append([image_file, np.array(cat_label)])\n                    if len(self) >= self.LIM:\n                        break\n                linecount += 6\n\n    def __getitem__(self, index):\n        image_file, label = self.indices[index]\n        try:\n            dcm = pydicom.dcmread(self.image_dir + os.sep + image_file)\n            img_arr = np.array(iu.rescale2d(np.array(dcm.pixel_array, dtype=np.int64)) * 255, dtype=np.uint8)\n            img_arr = iu.apply_clahe(img_arr)\n            \n            if self.transforms is not None:\n                img_arr = self.transforms(img_arr)\n            return {'inputs':img_arr, 'labels':label, 'index':index}\n        except Exception as e:\n            print('ERR!', str(e))\n\n    def __len__(self):\n        return len(self.indices)\n\n    @classmethod\n    def get_test_loader(cls, conf, shuffle_indices=False):\n        testset = cls(conf, 'test', test_transforms)\n        testset.image_dir = conf['Dirs']['test_image_dir']\n        testset.mapping_file = conf['test_mapping_file']\n        testset.load_data_indices(conf['validate_img_pth'])\n        return DataLoader(dataset=testset, batch_size=conf['Params']['batch_size'], shuffle=shuffle_indices,\n                          num_workers=5, drop_last=False)\n\n    @classmethod\n    def get_train_val_loader(cls, conf, shuffle_indices=True, drop_last_batch=True, split_ratio=[0.8, 0.2]):\n        full_dataset = cls(conf, 'train', train_transforms)\n        full_dataset.image_dir = conf['Dirs']['train_image_dir']\n        full_dataset.mapping_file = conf['train_mapping_file']\n        full_dataset.load_data_indices(conf['validate_img_pth'])\n        size_a = math.ceil(split_ratio[0] * len(full_dataset))\n        size_b = math.floor(split_ratio[1] * len(full_dataset))\n        dataset_a, dataset_b = random_split(full_dataset, [size_a, size_b])\n        loader_a = DataLoader(dataset_a,\n                              batch_size=conf['Params']['batch_size'], \n                              shuffle=shuffle_indices, num_workers=3, \n                              drop_last=drop_last_batch, collate_fn=clean_collate)\n        loader_b = DataLoader(dataset_b,\n                              batch_size=conf['Params']['batch_size'],\n                              shuffle=shuffle_indices, num_workers=3, \n                              drop_last=drop_last_batch, collate_fn=clean_collate)\n        return loader_a, loader_b","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# img_plot = images_arr.copy()\n# plt.tight_layout()\n# fig, axes = plt.subplots(4, 3, figsize=(10, 18), gridspec_kw = {'wspace':0.01, 'hspace':0.01})\n# for i in range(axes.shape[0]):\n#     for j in range(axes.shape[1]):\n#         axes[i, j].imshow(img_plot.pop(), 'gray')\n#         axes[i, j].set_xticklabels([])\n#         axes[i, j].set_yticklabels([])\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch import nn\n\n\nclass _DoubleConvolution(nn.Module):\n    def __init__(self, in_channels, middle_channel, out_channels, p=0):\n        super(_DoubleConvolution, self).__init__()\n        layers = [\n            nn.Conv2d(in_channels, middle_channel, kernel_size=3, padding=p),\n            nn.BatchNorm2d(middle_channel),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(middle_channel, out_channels, kernel_size=3, padding=p),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        ]\n        self.encode = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.encode(x)\n\n\nclass SkullNet(nn.Module):\n    def __init__(self, num_channels, num_classes):\n        super(SkullNet, self).__init__()\n        self.reduce_by = 2\n        self.num_classes = num_classes\n\n        self.C1 = _DoubleConvolution(num_channels, int(64 / self.reduce_by), int(64 / self.reduce_by))\n        self.C2 = _DoubleConvolution(int(64 / self.reduce_by), int(128 / self.reduce_by), int(128 / self.reduce_by))\n        self.C3 = _DoubleConvolution(int(128 / self.reduce_by), int(256 / self.reduce_by), int(256 / self.reduce_by))\n        self.C4 = _DoubleConvolution(int(256 / self.reduce_by), int(512 / self.reduce_by), int(256 / self.reduce_by))\n        self.C5 = _DoubleConvolution(int(256 / self.reduce_by), int(128 / self.reduce_by), int(128 / self.reduce_by))\n        self.C6 = _DoubleConvolution(int(128 / self.reduce_by), int(32 / self.reduce_by), 4)\n        self.fc1 = nn.Linear(4 * 8 * 8, 64)\n        self.fc2 = nn.Linear(64, 12)\n\n    def forward(self, x):\n        c1 = self.C1(x)\n        c1_mxp = F.max_pool2d(c1, kernel_size=2, stride=2)\n\n        c2 = self.C2(c1_mxp)\n        c2_mxp = F.max_pool2d(c2, kernel_size=2, stride=2)\n\n        c3 = self.C3(c2_mxp)\n        c3_mxp = F.max_pool2d(c3, kernel_size=2, stride=2)\n\n        c4 = self.C4(c3_mxp)\n        c4_mxp = F.max_pool2d(c4, kernel_size=2, stride=2)\n\n        c5 = self.C5(c4_mxp)\n        c5_mxp = F.max_pool2d(c5, kernel_size=2, stride=2)\n\n        c6 = self.C6(c5_mxp)\n\n        fc1 = self.fc1(c6.view(-1, 4 * 8 * 8))\n        fc2 = self.fc2(fc1)\n        out = fc2.view(fc2.shape[0], 2, -1)\n        return out\n\n    @staticmethod\n    def match_and_concat(bypass, upsampled, crop=True):\n        if crop:\n            c = (bypass.size()[2] - upsampled.size()[2]) // 2\n            bypass = F.pad(bypass, (-c, -c, -c, -c))\n        return torch.cat((upsampled, bypass), 1)\n\n\nm = SkullNet(1, 2)\ntorch_total_params = sum(p.numel() for p in m.parameters() if p.requires_grad)\nprint('Total Params:', torch_total_params)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train Validation and Test Module"},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\n\nimport torch.nn.functional as F\n\n\nclass NNTrainer:\n\n    def __init__(self, conf=None, model=None, optimizer=None):\n\n        # Initialize parameters and directories before-hand so that we can clearly track which ones are used\n        self.conf = conf\n        self.log_dir = self.conf.get('Dirs').get('logs', 'net_logs')\n        self.epochs = self.conf.get('Params').get('epochs', 100)\n        self.log_frequency = self.conf.get('Params').get('log_frequency', 10)\n        self.validation_frequency = self.conf.get('Params').get('validation_frequency', 1)\n        self.mode = self.conf.get('Params').get('mode', 'test')\n\n        # Initialize necessary logging conf\n        self.checkpoint_file = os.path.join(self.log_dir, self.conf.get('checkpoint_file'))\n\n        self.log_headers = self.get_log_headers()\n        _log_key = self.conf.get('checkpoint_file').split('.')[0]\n        self.test_logger = NNTrainer.get_logger(log_file=os.path.join(self.log_dir, 'submission.csv'),\n                                                header=self.log_headers.get('test', ''))\n        if self.mode == 'train':\n            self.train_logger = NNTrainer.get_logger(log_file=os.path.join(self.log_dir, _log_key + '-TRAIN.csv'),\n                                                     header=self.log_headers.get('train', ''))\n            self.val_logger = NNTrainer.get_logger(log_file=os.path.join(self.log_dir, _log_key + '-VAL.csv'),\n                                                   header=self.log_headers.get('validation', ''))\n\n        self.f_dyn_weights = self.conf.get('f_dyn_weights')\n\n        # Handle gpu/cpu\n        if torch.cuda.is_available():\n            self.device = torch.device(\"cuda\" if self.conf['Params'].get('use_gpu', False) else \"cpu\")\n        else:\n            print('### GPU not found.')\n            self.device = torch.device(\"cpu\")\n\n        # Initialization to save model\n        self.model = model.to(self.device)\n        self.optimizer = optimizer\n        self.checkpoint = {'total_epochs:': 0, 'epochs': 0, 'state': None, 'score': 0.0, 'model': 'EMPTY'}\n        self.patience = self.conf.get('Params').get('patience', 35)\n\n    def test(self, data_loaders=None):\n        print('------Running test------')\n        score = ScoreAccumulator()\n        self.model.eval()\n        with torch.no_grad():\n            for i, data in enumerate(data_loaders, 1):\n                inputs, labels = data['inputs'].to(self.device).float(), data['labels'].to(self.device).long()\n\n                if self.model.training:\n                    self.optimizer.zero_grad()\n\n                outputs = self.model(inputs)\n                for ix, pred  in zip(data['index'], F.softmax(outputs, 1)[:,1,:]):\n                    file = data_loaders.dataset.indices[ix][0].split('.')[0]\n\n                    p_EPIDURAL = pred[0].item()\n                    p_INTRAPARENCHYMAL = pred[1].item()\n                    p_INTRAVENTRICULAR = pred[2].item()\n                    p_SUBARACHNOID = pred[3].item()\n                    p_SUBDURAL = pred[4].item()\n                    p_ANY = pred[5].item()\n\n                    log = file + '_' + EPIDURAL + ',' + str(p_EPIDURAL)\n                    log += '\\n' + file + '_' + INTRAPARENCHYMAL + ',' + str(p_INTRAPARENCHYMAL)\n                    log += '\\n' + file + '_' + INTRAVENTRICULAR + ',' + str(p_INTRAVENTRICULAR)\n                    log += '\\n' + file + '_' + SUBARACHNOID + ',' + str(p_SUBARACHNOID)\n                    log += '\\n' + file + '_' + SUBDURAL + ',' + str(p_SUBDURAL) \n                    log += '\\n' + file + '_' + ANY + ',' + str(p_ANY)\n                    NNTrainer.flush(self.test_logger, log)\n                    print('{}/{} test batch processed.'.format(i, len(data_loaders)), end='\\r')\n\n        self._on_test_end(log_file=self.test_logger.name)\n        if not self.test_logger and not self.test_logger.closed:\n            self.test_logger.close()\n\n    def train(self, data_loader=None, validation_loader=None, epoch_run=None):\n        print('Training...')\n        for epoch in range(1, self.epochs + 1):\n            self.model.train()\n            self._adjust_learning_rate(epoch=epoch)\n            self.checkpoint['total_epochs'] = epoch\n\n            # Run one epoch\n            epoch_run(epoch=epoch, data_loader=data_loader, logger=self.train_logger)\n\n            self._on_epoch_end(data_loader=data_loader, log_file=self.train_logger.name)\n\n            # Validation_frequency is the number of epoch until validation\n            if epoch % self.validation_frequency == 0:\n                print('############# Running validation... ####################')\n                self.model.eval()\n                with torch.no_grad():\n                    self.validation(epoch=epoch, validation_loader=validation_loader, epoch_run=epoch_run)\n                self._on_validation_end(data_loader=validation_loader, log_file=self.val_logger.name)\n                if self.early_stop(patience=self.patience):\n                    return\n                print('########################################################')\n\n        if not self.train_logger and not self.train_logger.closed:\n            self.train_logger.close()\n        if not self.val_logger and not self.val_logger.closed:\n            self.val_logger.close()\n\n    def _on_epoch_end(self, **kw):\n        viz.plot_column_keys(file=kw['log_file'], batches_per_epoch=kw['data_loader'].__len__(),\n                              keys=['F1', 'LOSS', 'ACCURACY'])\n        viz.plot_cmap(file=kw['log_file'], save=True, x='PRECISION', y='RECALL')\n\n    def _on_validation_end(self, **kw):\n        viz.plot_column_keys(file=kw['log_file'], batches_per_epoch=kw['data_loader'].__len__(),\n                              keys=['F1', 'ACCURACY'])\n        viz.plot_cmap(file=kw['log_file'], save=True, x='PRECISION', y='RECALL')\n\n    def _on_test_end(self, **kw):\n        viz.y_scatter(file=kw['log_file'], y='F1', label='ID', save=True, title='Test')\n        viz.y_scatter(file=kw['log_file'], y='ACCURACY', label='ID', save=True, title='Test')\n        viz.xy_scatter(file=kw['log_file'], save=True, x='PRECISION', y='RECALL', label='ID', title='Test')\n\n    # Headers for log files\n    def get_log_headers(self):\n        return {\n            'train': 'ID,EPOCH,BATCH,PRECISION,RECALL,F1,ACCURACY,LOSS',\n            'validation': 'ID,PRECISION,RECALL,F1,ACCURACY',\n            'test': 'ID,Label'\n        }\n    \n    def validation(self, epoch=None, validation_loader=None, epoch_run=None):\n        score_acc = ScoreAccumulator()\n        epoch_run(epoch=epoch, data_loader=validation_loader, logger=self.val_logger, score_acc=score_acc)\n        p, r, f1, a = score_acc.get_prfa()\n        print('>>> PRF1: ', [p, r, f1, a])\n        self._save_if_better(score=f1)\n\n    def resume_from_checkpoint(self, parallel_trained=False):\n        self.checkpoint = torch.load(self.checkpoint_file)\n        print(self.checkpoint_file, 'Loaded...')\n        try:\n            if parallel_trained:\n                from collections import OrderedDict\n                new_state_dict = OrderedDict()\n                for k, v in self.checkpoint['state'].items():\n                    name = k[7:]  # remove `module.`\n                    new_state_dict[name] = v\n                # load params\n                self.model.load_state_dict(new_state_dict)\n            else:\n                self.model.load_state_dict(self.checkpoint['state'])\n        except Exception as e:\n            print('ERROR: ' + str(e))\n\n    def _save_if_better(self, score=None):\n\n        if self.mode == 'test':\n            return\n\n        if score > self.checkpoint['score']:\n            print('Score improved: ',\n                  str(self.checkpoint['score']) + ' to ' + str(score) + ' BEST CHECKPOINT SAVED')\n            self.checkpoint['state'] = self.model.state_dict()\n            self.checkpoint['epochs'] = self.checkpoint['total_epochs']\n            self.checkpoint['score'] = score\n            self.checkpoint['model'] = str(self.model)\n            torch.save(self.checkpoint, self.checkpoint_file)\n        else:\n            print('Score did not improve:' + str(score) + ' BEST: ' + str(self.checkpoint['score']) + ' Best EP: ' + (\n                str(self.checkpoint['epochs'])))\n\n    def early_stop(self, patience=35):\n        return self.checkpoint['total_epochs'] - self.checkpoint['epochs'] >= patience * self.validation_frequency\n\n    @staticmethod\n    def get_logger(log_file=None, header=''):\n\n        if os.path.isfile(log_file):\n            print('### CRITICAL!!! ' + log_file + '\" already exists.')\n            ip = input('Override? [Y/N]: ')\n            if ip == 'N' or ip == 'n':\n                sys.exit(1)\n\n        file = open(log_file, 'w')\n        NNTrainer.flush(file, header)\n        return file\n\n    @staticmethod\n    def flush(logger, msg):\n        if logger is not None:\n            logger.write(msg + '\\n')\n            logger.flush()\n\n    def _adjust_learning_rate(self, epoch):\n        if epoch % 30 == 0:\n            for param_group in self.optimizer.param_groups:\n                if param_group['lr'] >= 1e-5:\n                    param_group['lr'] = param_group['lr'] * 0.7\n        \n    def epoch_ce_loss(self, **kw):\n        \"\"\"\n        One epoch implementation of binary cross-entropy loss\n        :param kw:\n        :return:\n        \"\"\"\n        running_loss = 0.0\n        score_acc = ScoreAccumulator() if self.model.training else kw.get('score_acc')\n        assert isinstance(score_acc, ScoreAccumulator)\n\n        for i, data in enumerate(kw['data_loader'], 1):\n            inputs, labels = data['inputs'].to(self.device).float(), data['labels'].to(self.device).long()\n            \n            if self.model.training:\n                self.optimizer.zero_grad()\n\n            outputs = self.model(inputs)\n            _, predicted = torch.max(outputs, 1)\n            loss = F.nll_loss(F.log_softmax(outputs, 1), labels, weight=torch.FloatTensor(self.f_dyn_weights(self.conf)).to(self.device))\n            \n            if self.model.training:\n                loss.backward()\n                self.optimizer.step()\n\n            current_loss = loss.item()\n            running_loss += current_loss\n\n            if self.model.training:\n                score_acc.reset()\n\n            p, r, f1, a = score_acc.add_tensor(predicted, labels).get_prfa()\n\n            if i % self.log_frequency == 0:\n                print('Epochs[%d/%d] Batch[%d/%d] loss:%.5f pre:%.3f rec:%.3f f1:%.3f acc:%.3f' %\n                      (\n                          kw['epoch'], self.epochs, i, kw['data_loader'].__len__(),\n                          running_loss / self.log_frequency, p, r, f1,\n                          a))\n                running_loss = 0.0\n            self.flush(kw['logger'],\n                       ','.join(str(x) for x in [0, kw['epoch'], i, p, r, f1, a, current_loss]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training setup"},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\n### author: Aashis Khanal\n### sraashis@gmail.com\n### date: 9/10/2018\n\"\"\"\n\nimport os\nimport traceback\n\nimport torch\nimport torch.optim as optim\n\n\ndef run(runs):\n    for R in runs:\n        for k, folder in R['Dirs'].items():\n            os.makedirs(folder, exist_ok=True)\n        R['acc'] = ScoreAccumulator()\n        R['checkpoint_file'] = R['train_mapping_file'].split(os.sep)[1] + '.tar'\n        model = SkullNet(R['Params']['num_channels'], R['Params']['num_classes'])\n        optimizer = optim.Adam(model.parameters(), lr=R['Params']['learning_rate'])\n        if R['Params']['distribute']:\n            model = torch.nn.DataParallel(model)\n            model.float()\n            optimizer = optim.Adam(model.module.parameters(), lr=R['Params']['learning_rate'])\n\n        try:\n            trainer = NNTrainer(model=model, conf=R, optimizer=optimizer)\n            if R.get('Params').get('mode') == 'train':\n                train_loader, val_loader = SkullDataset.get_train_val_loader(R)\n                print('### Train Val Batch size:', len(train_loader.dataset), len(val_loader.dataset))\n                trainer.train(data_loader=train_loader, validation_loader=val_loader,\n                              epoch_run=trainer.epoch_ce_loss)\n\n            test_loader = SkullDataset.get_test_loader(conf=R)\n            trainer.resume_from_checkpoint(parallel_trained=R.get('Params').get('parallel_trained'))\n            \n            trainer.test(test_loader)\n        except Exception as e:\n            traceback.print_exc()\n\n    print(R['acc'].get_prfa())\n    f = open(R['Dirs']['logs'] + os.sep + 'score.txt', \"w\")\n    f.write(', '.join(str(s) for s in R['acc'].get_prfa()))\n    f.close()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import FileLink\nFileLink('logs/submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Params = {\n    'num_channels': 1,\n    'num_classes': 2,\n    'batch_size': 32,\n    'epochs': 3,\n    'learning_rate': 0.001,\n    'use_gpu': True,\n    'distribute': True,\n    'shuffle': True,\n    'log_frequency': 10,\n    'validation_frequency': 1,\n    'mode': 'train',\n    'parallel_trained': False,\n}\nSKDB = {\n    'Params': Params,\n    'Dirs': {\n        'train_image_dir': train_images_dir,\n        'test_image_dir': test_images_dir,\n        'logs': 'logs'},\n    'train_mapping_file': train_mapping_file,\n    'test_mapping_file': test_mapping_file,\n    'f_dyn_weights': lambda x: np.random.choice(np.arange(1, 1001, 1), 2),\n    'validate_img_pth': False,\n    'load_lim': 10e10\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# run([SKDB])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"pycharm":{"stem_cell":{"cell_type":"raw","source":[],"metadata":{"collapsed":false}}}},"nbformat":4,"nbformat_minor":1}
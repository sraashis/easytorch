{
 "cells": [
  {
   "metadata": {
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "import math\n",
    "import os\n",
    "from itertools import islice\n",
    "sep = os.sep\n",
    "import numpy as np\n",
    "import pydicom\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data.dataset import Dataset, random_split\n",
    "import torchvision.transforms as tmf\n",
    "import pandas as pd\n",
    "import random\n",
    "from argparse import Namespace\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "import json"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "import img_utils as iu\n",
    "import nnviz as viz\n",
    "from measurements import ScoreAccumulator\n",
    "from torchutils import NNTrainer, NNDataLoader"
   ],
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "train_mapping_file = '../input/rsna-intracranial-hemorrhage-detection/stage_1_train.csv'\n",
    "train_images_dir = '../input/rsna-intracranial-hemorrhage-detection/stage_1_train_images/'\n",
    "test_mapping_file = '../input/rsna-intracranial-hemorrhage-detection/stage_1_sample_submission.csv'\n",
    "test_images_dir = '../input/rsna-intracranial-hemorrhage-detection/stage_1_test_images/'"
   ],
   "execution_count": 6,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "transforms = tmf.Compose([\n",
    "    tmf.ToPILImage(),\n",
    "    tmf.Resize((512, 512), interpolation=2),\n",
    "    tmf.RandomHorizontalFlip(),\n",
    "    tmf.RandomVerticalFlip(),\n",
    "    tmf.ToTensor()\n",
    "])"
   ],
   "execution_count": 7,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "test_transforms = tmf.Compose([\n",
    "    tmf.ToPILImage(),\n",
    "    tmf.Resize((512, 512), interpolation=2),\n",
    "    tmf.ToTensor()\n",
    "])"
   ],
   "execution_count": 8,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data._utils.collate import default_collate\n",
    "def clean_collate(batch):\n",
    "    return default_collate([b for b in batch if b])"
   ],
   "execution_count": 9,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "class SkullDataset(Dataset):\n",
    "    def __init__(self, transforms=None, mode=None, load_lim=np.inf):\n",
    "        self.transforms = transforms\n",
    "        self.mode = mode\n",
    "        self.image_dir = None\n",
    "        self.mapping_file = None\n",
    "        self.indices = []\n",
    "        self.LIM = load_lim\n",
    "\n",
    "    def load_data_indices(self, reindex=True):\n",
    "        print(self.mapping_file, '...')\n",
    "        with open(self.mapping_file) as infile:\n",
    "            linecount, six_rows, _ = 1, True, next(infile)\n",
    "            while six_rows and len(self) < self.LIM:\n",
    "                try:\n",
    "                    print('Reading Line: {}'.format(linecount), end='\\r')\n",
    "\n",
    "                    six_rows = list(r.rstrip().split(',') for r in islice(infile, 6))\n",
    "                    image_file, cat_label = None, []\n",
    "                    for hname, label in six_rows:\n",
    "                        (ID, file_ID, htype), label = hname.split('_'), float(label)\n",
    "                        image_file = ID + '_' + file_ID + '.dcm'\n",
    "                        cat_label.append(label)\n",
    "\n",
    "                    if image_file and len(cat_label) == 6:\n",
    "                        self.indices.append([image_file, np.array(cat_label)])\n",
    "                    \n",
    "                    linecount += 6\n",
    "                except Exception as e:\n",
    "                    traceback.print_exc()\n",
    "    \n",
    "    def equalize_index(self, shuffle=True):\n",
    "        self.indices0 = []\n",
    "        self.indices1 = []\n",
    "        for file, lbl in self.indices:\n",
    "            if np.sum(lbl) == 0:\n",
    "                self.indices0.append([file, lbl])\n",
    "            else:\n",
    "                self.indices1.append([file, lbl])\n",
    "                \n",
    "        if shuffle:            \n",
    "            random.shuffle(self.indices0)\n",
    "            random.shuffle(self.indices1)\n",
    "        self.indices = self.indices1 + self.indices0[0:len(self.indices1)]\n",
    "        print('Items After Equalize Reindex: ', len(self))\n",
    "                \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_file, label = self.indices[index]\n",
    "        try:\n",
    "            dcm = pydicom.dcmread(self.image_dir + os.sep + image_file)\n",
    "            img_arr = np.array(iu.rescale2d(np.array(dcm.pixel_array, dtype=np.int64)) * 255, dtype=np.uint8)\n",
    "            img_arr = iu.apply_clahe(img_arr)\n",
    "            \n",
    "            if self.transforms is not None:\n",
    "                img_arr = self.transforms(img_arr)\n",
    "            return {'inputs':img_arr, 'labels':label, 'index':index}\n",
    "        except Exception as e:\n",
    "            traceback.print_exc()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "        \n",
    "    @classmethod\n",
    "    def get_test_set(cls, conf, transforms):\n",
    "        testset = cls(transforms, 'test', conf['load_lim'])\n",
    "        testset.image_dir = conf['test_image_dir']\n",
    "        testset.mapping_file = conf['test_mapping_file']\n",
    "        testset.load_data_indices()\n",
    "        return testset\n",
    "\n",
    "    @classmethod\n",
    "    def get_train_val_set(cls, conf, train_transforms, val_transforms, split_ratio=[0.8, 0.2]):\n",
    "        full_dataset = cls(transforms, 'full', conf['load_lim'])\n",
    "        full_dataset.image_dir = conf['train_image_dir']\n",
    "        full_dataset.mapping_file = conf['train_mapping_file']\n",
    "        full_dataset.load_data_indices()\n",
    "        full_dataset.equalize_index()\n",
    "        sz = math.ceil(split_ratio[0] * len(full_dataset))\n",
    "        \n",
    "        trainset = cls(train_transforms, 'train')\n",
    "        trainset.indices = full_dataset.indices[0:sz]\n",
    "        trainset.image_dir = conf['train_image_dir']\n",
    "        \n",
    "        valset =  cls(val_transforms, 'validation')\n",
    "        valset.indices = full_dataset.indices[sz:len(full_dataset)]\n",
    "        valset.image_dir = conf['train_image_dir']\n",
    "        \n",
    "        return trainset, valset"
   ],
   "execution_count": 50,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "# img_plot = images_arr.copy()\n",
    "# plt.tight_layout()\n",
    "# fig, axes = plt.subplots(4, 3, figsize=(10, 18), gridspec_kw = {'wspace':0.01, 'hspace':0.01})\n",
    "# for i in range(axes.shape[0]):\n",
    "#     for j in range(axes.shape[1]):\n",
    "#         axes[i, j].imshow(img_plot.pop(), 'gray')\n",
    "#         axes[i, j].set_xticklabels([])\n",
    "#         axes[i, j].set_yticklabels([])\n",
    "# plt.show()"
   ],
   "execution_count": 32,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Model"
   ]
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class _DoubleConvolution(nn.Module):\n",
    "    def __init__(self, in_channels, middle_channel, out_channels, p=0):\n",
    "        super(_DoubleConvolution, self).__init__()\n",
    "        layers = [\n",
    "            nn.Conv2d(in_channels, middle_channel, kernel_size=3, padding=p),\n",
    "            nn.BatchNorm2d(middle_channel),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(middle_channel, out_channels, kernel_size=3, padding=p),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ]\n",
    "        self.encode = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encode(x)\n",
    "\n",
    "\n",
    "class SkullNet(nn.Module):\n",
    "    def __init__(self, num_channels, num_classes):\n",
    "        super(SkullNet, self).__init__()\n",
    "        self.reduce_by = 2\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.C1 = _DoubleConvolution(num_channels, int(64 / self.reduce_by), int(64 / self.reduce_by))\n",
    "        self.C2 = _DoubleConvolution(int(64 / self.reduce_by), int(128 / self.reduce_by), int(128 / self.reduce_by))\n",
    "        self.C3 = _DoubleConvolution(int(128 / self.reduce_by), int(256 / self.reduce_by), int(256 / self.reduce_by))\n",
    "        self.C4 = _DoubleConvolution(int(256 / self.reduce_by), int(512 / self.reduce_by), int(256 / self.reduce_by))\n",
    "        self.C5 = _DoubleConvolution(int(256 / self.reduce_by), int(128 / self.reduce_by), int(128 / self.reduce_by))\n",
    "        self.C6 = _DoubleConvolution(int(128 / self.reduce_by), int(32 / self.reduce_by), 4)\n",
    "        self.fc1 = nn.Linear(4 * 8 * 8, 64)\n",
    "        self.fc2 = nn.Linear(64, 12)\n",
    "\n",
    "    def forward(self, x):\n",
    "        c1 = self.C1(x)\n",
    "        c1_mxp = F.max_pool2d(c1, kernel_size=2, stride=2)\n",
    "\n",
    "        c2 = self.C2(c1_mxp)\n",
    "        c2_mxp = F.max_pool2d(c2, kernel_size=2, stride=2)\n",
    "\n",
    "        c3 = self.C3(c2_mxp)\n",
    "        c3_mxp = F.max_pool2d(c3, kernel_size=2, stride=2)\n",
    "\n",
    "        c4 = self.C4(c3_mxp)\n",
    "        c4_mxp = F.max_pool2d(c4, kernel_size=2, stride=2)\n",
    "\n",
    "        c5 = self.C5(c4_mxp)\n",
    "        c5_mxp = F.max_pool2d(c5, kernel_size=2, stride=2)\n",
    "\n",
    "        c6 = self.C6(c5_mxp)\n",
    "\n",
    "        fc1 = self.fc1(c6.view(-1, 4 * 8 * 8))\n",
    "        fc2 = self.fc2(fc1)\n",
    "        out = fc2.view(fc2.shape[0], 2, -1)\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def match_and_concat(bypass, upsampled, crop=True):\n",
    "        if crop:\n",
    "            c = (bypass.size()[2] - upsampled.size()[2]) // 2\n",
    "            bypass = F.pad(bypass, (-c, -c, -c, -c))\n",
    "        return torch.cat((upsampled, bypass), 1)\n",
    "\n",
    "\n",
    "m = SkullNet(1, 2)\n",
    "torch_total_params = sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
    "print('Total Params:', torch_total_params)"
   ],
   "execution_count": 33,
   "outputs": [
    {
     "output_type": "stream",
     "text": "Total Params: 1016360\n",
     "name": "stdout"
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Train Validation and Test Module"
   ]
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "EPIDURAL = 'epidural'\n",
    "INTRAPARENCHYMAL = 'intraparenchymal'\n",
    "INTRAVENTRICULAR = 'intraventricular'\n",
    "SUBARACHNOID = 'subarachnoid'\n",
    "SUBDURAL = 'subdural'\n",
    "ANY = 'any'\n",
    "class SkullTrainer(NNTrainer):\n",
    "    def __init__(self, **kw):\n",
    "        super(SkullTrainer, self).__init__(**kw)\n",
    "\n",
    "    def test(self, testset=None):\n",
    "        print('------Running test------')\n",
    "        testloader = NNDataLoader.get_loader(testset, **self.conf)\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(testloader, 1):\n",
    "                inputs, labels = data['inputs'].to(self.device).float(), data['labels'].to(self.device).long()\n",
    "\n",
    "                if self.model.training:\n",
    "                    self.optimizer.zero_grad()\n",
    "\n",
    "                outputs = self.model(inputs)\n",
    "                for ix, pred in zip(data['index'], F.softmax(outputs, 1)[:, 1, :]):\n",
    "                    file = testset.indices[ix][0].split('.')[0]\n",
    "\n",
    "                    p_EPIDURAL = pred[0].item()\n",
    "                    p_INTRAPARENCHYMAL = pred[1].item()\n",
    "                    p_INTRAVENTRICULAR = pred[2].item()\n",
    "                    p_SUBARACHNOID = pred[3].item()\n",
    "                    p_SUBDURAL = pred[4].item()\n",
    "                    p_ANY = pred[5].item()\n",
    "\n",
    "                    log = file + '_' + EPIDURAL + ',' + str(p_EPIDURAL)\n",
    "                    log += '\\n' + file + '_' + INTRAPARENCHYMAL + ',' + str(p_INTRAPARENCHYMAL)\n",
    "                    log += '\\n' + file + '_' + INTRAVENTRICULAR + ',' + str(p_INTRAVENTRICULAR)\n",
    "                    log += '\\n' + file + '_' + SUBARACHNOID + ',' + str(p_SUBARACHNOID)\n",
    "                    log += '\\n' + file + '_' + SUBDURAL + ',' + str(p_SUBDURAL)\n",
    "                    log += '\\n' + file + '_' + ANY + ',' + str(p_ANY)\n",
    "                    NNTrainer.flush(self.test_logger, log)\n",
    "                    print('{}/{} test batch processed.'.format(i, len(testloader)), end='\\r')\n",
    "\n",
    "    def one_epoch_run(self, **kw):\n",
    "        \"\"\"\n",
    "        One epoch implementation of binary cross-entropy loss\n",
    "        :param kw:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        running_loss = 0.0\n",
    "        score_acc = ScoreAccumulator() if self.model.training else kw.get('score_accumulator')\n",
    "        assert isinstance(score_acc, ScoreAccumulator)\n",
    "        for i, data in enumerate(kw['data_loader'], 1):\n",
    "            inputs, labels = data['inputs'].to(self.device).float(), data['labels'].to(self.device).long()\n",
    "\n",
    "            if self.model.training:\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "            outputs = self.model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            _wt = None\n",
    "            if self.cls_weights:\n",
    "                _wt = torch.FloatTensor(self.cls_weights(self.conf)).to(self.device)\n",
    "\n",
    "            loss = F.nll_loss(F.log_softmax(outputs, 1), labels, weight=_wt)\n",
    "\n",
    "            if self.model.training:\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            current_loss = loss.item()\n",
    "            running_loss += current_loss\n",
    "\n",
    "            if self.model.training:\n",
    "                score_acc.reset()\n",
    "\n",
    "            p, r, f1, a = score_acc.add_tensor(predicted, labels).get_prfa()\n",
    "\n",
    "            if i % self.log_frequency == 0:\n",
    "                print('Epochs[%d/%d] Batch[%d/%d] loss:%.5f pre:%.3f rec:%.3f f1:%.3f acc:%.3f' %\n",
    "                      (\n",
    "                          kw['epoch'], self.epochs, i, kw['data_loader'].__len__(),\n",
    "                          running_loss / self.log_frequency, p, r, f1,\n",
    "                          a))\n",
    "                running_loss = 0.0\n",
    "            self.flush(kw['logger'],\n",
    "                       ','.join(str(x) for x in [0, kw['epoch'], i, p, r, f1, a, current_loss]))"
   ],
   "execution_count": 34,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Training setup"
   ]
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "### author: Aashis Khanal\n",
    "### sraashis@gmail.com\n",
    "### date: 9/10/2018\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import traceback\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "def run(R):\n",
    "    model = SkullNet(R['input_channels'], R['num_classes'])\n",
    "    optimizer = optim.Adam(model.parameters(), lr=R['learning_rate'])\n",
    "    if R['distribute']:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "        model.float()\n",
    "        optimizer = optim.Adam(model.module.parameters(), lr=R['learning_rate'])\n",
    "\n",
    "    try:\n",
    "        trainer = SkullTrainer(model=model, conf=R, optimizer=optimizer)\n",
    "        if R.get('mode') == 'train':\n",
    "            trainset, valset = SkullDataset.get_train_val_set(R, transforms, transforms)\n",
    "            print('### Train Val Batch size:', len(trainset), len(valset))\n",
    "            trainer.train(trainset, valset)\n",
    "\n",
    "        testset = SkullDataset.get_test_set(R, transforms)\n",
    "        trainer.resume_from_checkpoint(parallel_trained=R.get('parallel_trained'))\n",
    "\n",
    "        trainer.test(testset)\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()"
   ],
   "execution_count": 35,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "# os.listdir('net_logs/')"
   ],
   "execution_count": 36,
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 36,
     "data": {
      "text/plain": "['checkpoint.tar',\n 'checkpoint-VAL-_F1.png',\n 'checkpoint-TRAIN-_ACCURACY.png',\n 'checkpoint-VAL.csv',\n 'checkpoint-VAL-_PRECISION_RECALL_cmap.png',\n 'checkpoint-TEST.csv',\n 'checkpoint-VAL-_ACCURACY.png',\n 'checkpoint-TRAIN-_LOSS.png',\n 'checkpoint-TRAIN-_PRECISION_RECALL_cmap.png',\n 'checkpoint-TRAIN-_F1.png',\n 'checkpoint-TRAIN.csv']"
     },
     "metadata": {}
    }
   ]
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "from IPython.display import FileLink\n# FileLink('net_logs/submission.csv')",
   "execution_count": 37,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "SKDB = {\n    'input_channels': 1,\n    'num_classes': 2,\n    'batch_size': 32,\n    'epochs': 1,\n    'learning_rate': 0.001,\n    'use_gpu': True,\n    'distribute': True,\n    'shuffle': True,\n    'log_frequency': 10,\n    'validation_frequency': 1,\n    'parallel_trained': False,\n    'num_workers':3,\n    'train_image_dir': train_images_dir,\n    'test_image_dir': test_images_dir,\n    'train_mapping_file': train_mapping_file,\n    'test_mapping_file': test_mapping_file,\n    'checkpoint_file': 'checkpoint.tar',\n    'cls_weights': lambda x: np.random.choice(np.arange(1, 101, 1), 2),\n    'mode': 'train',\n    'load_lim': np.inf\n}",
   "execution_count": 51,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# run(SKDB)",
   "execution_count": 52,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "version": "3.6.4",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python",
   "mimetype": "text/x-python"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
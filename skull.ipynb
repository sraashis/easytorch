{
 "cells": [
  {
   "metadata": {
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "import math\n",
    "import os\n",
    "from itertools import islice\n",
    "sep = os.sep\n",
    "import numpy as np\n",
    "import pydicom\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data.dataset import Dataset, random_split\n",
    "import torchvision.transforms as tmf"
   ],
   "execution_count": 7,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "import img_utils as iu\n",
    "import nnviz as viz\n",
    "from measurements import ScoreAccumulator"
   ],
   "execution_count": 8,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "EPIDURAL = 'epidural'\n",
    "INTRAPARENCHYMAL = 'intraparenchymal'\n",
    "INTRAVENTRICULAR = 'intraventricular'\n",
    "SUBARACHNOID = 'subarachnoid'\n",
    "SUBDURAL = 'subdural'\n",
    "ANY = 'any'"
   ],
   "execution_count": 9,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "train_mapping_file = '../input/rsna-intracranial-hemorrhage-detection/stage_1_train.csv'\n",
    "train_images_dir = '../input/rsna-intracranial-hemorrhage-detection/stage_1_train_images/'\n",
    "test_mapping_file = '../input/rsna-intracranial-hemorrhage-detection/stage_1_sample_submission.csv'\n",
    "test_images_dir = '../input/rsna-intracranial-hemorrhage-detection/stage_1_test_images/'"
   ],
   "execution_count": 10,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "train_transforms = tmf.Compose([\n",
    "    tmf.ToPILImage(),\n",
    "    tmf.Resize((512, 512), interpolation=2),\n",
    "    tmf.RandomHorizontalFlip(),\n",
    "    tmf.RandomVerticalFlip(),\n",
    "    tmf.ToTensor()\n",
    "])"
   ],
   "execution_count": 21,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "test_transforms = tmf.Compose([\n",
    "    tmf.ToPILImage(),\n",
    "    tmf.Resize((512, 512), interpolation=2),\n",
    "    tmf.ToTensor()\n",
    "])"
   ],
   "execution_count": 22,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "class SkullDataset(Dataset):\n",
    "    def __init__(self, conf=None, mode=None, transforms=None):\n",
    "        self.transforms = transforms\n",
    "        self.mode = mode\n",
    "        self.conf = conf\n",
    "        self.image_dir = None\n",
    "        self.mapping_file = None\n",
    "        self.expand_by = self.conf.get('Params').get('expand_patch_by')\n",
    "        self.indices = []\n",
    "        self.LIM = conf.get('load_lim', 10e10)\n",
    "\n",
    "    def load_data_indices(self, validate_pth=False):\n",
    "        with open(self.mapping_file) as infile:\n",
    "            linecount, six_rows, _ = 1, True, next(infile)\n",
    "            while six_rows:\n",
    "                print('Reading Line: {}'.format(linecount), end='\\r')\n",
    "\n",
    "                six_rows = list(r.rstrip().split(',') for r in islice(infile, 6))\n",
    "                image_file, cat_label = None, []\n",
    "                for hname, label in six_rows:\n",
    "                    (ID, file_ID, htype), label = hname.split('_'), int(label if self.mode == 'train' else 0)\n",
    "                    fname_ = ID + '_' + file_ID + '.dcm'\n",
    "\n",
    "                    if validate_pth and not os.path.exists(os.path.join(self.image_dir, fname_)):\n",
    "                        break\n",
    "\n",
    "                    if image_file and fname_ != image_file:\n",
    "                        print('Mismatch Line: {}'.format(linecount), end='\\r')\n",
    "                        break\n",
    "                    else:\n",
    "                        image_file = fname_\n",
    "\n",
    "                    cat_label.append(label)\n",
    "\n",
    "                if image_file and len(cat_label) == 6:\n",
    "                    self.indices.append([image_file, np.array(cat_label)])\n",
    "                    if len(self) >= self.LIM:\n",
    "                        break\n",
    "                linecount += 6\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_file, label = self.indices[index]\n",
    "\n",
    "        dcm = pydicom.dcmread(self.image_dir + os.sep + image_file)\n",
    "        img_arr = np.array(iu.rescale2d_unsigned(dcm.pixel_array) * 255, dtype=np.uint8)\n",
    "        img_arr = iu.apply_clahe(img_arr)\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img_arr = self.transforms(img_arr)\n",
    "            \n",
    "        return {'inputs':img_arr, 'labels':label, 'index':index}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    @classmethod\n",
    "    def get_test_loader(cls, conf, shuffle_indices=False):\n",
    "        testset = cls(conf, 'test', test_transforms)\n",
    "        testset.image_dir = conf['Dirs']['test_image_dir']\n",
    "        testset.mapping_file = conf['test_mapping_file']\n",
    "        testset.load_data_indices(conf['validate_img_pth'])\n",
    "        return DataLoader(dataset=testset, batch_size=conf['Params']['batch_size'], shuffle=shuffle_indices,\n",
    "                          num_workers=5, drop_last=False)\n",
    "\n",
    "    @classmethod\n",
    "    def get_train_val_loader(cls, conf, shuffle_indices=True, drop_last_batch=True, split_ratio=[0.8, 0.2]):\n",
    "        full_dataset = cls(conf, 'train', train_transforms)\n",
    "        full_dataset.image_dir = conf['Dirs']['train_image_dir']\n",
    "        full_dataset.mapping_file = conf['train_mapping_file']\n",
    "        full_dataset.load_data_indices(conf['validate_img_pth'])\n",
    "        size_a = math.ceil(split_ratio[0] * len(full_dataset))\n",
    "        size_b = math.floor(split_ratio[1] * len(full_dataset))\n",
    "        dataset_a, dataset_b = random_split(full_dataset, [size_a, size_b])\n",
    "        loader_a = DataLoader(dataset_a,\n",
    "                              batch_size=conf['Params']['batch_size'], \n",
    "                              shuffle=shuffle_indices, num_workers=3, \n",
    "                              drop_last=drop_last_batch)\n",
    "        loader_b = DataLoader(dataset_b,\n",
    "                              batch_size=conf['Params']['batch_size'],\n",
    "                              shuffle=shuffle_indices, num_workers=3, \n",
    "                              drop_last=drop_last_batch)\n",
    "        return loader_a, loader_b"
   ],
   "execution_count": 24,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "# img_plot = images_arr.copy()\n",
    "# plt.tight_layout()\n",
    "# fig, axes = plt.subplots(4, 3, figsize=(10, 18), gridspec_kw = {'wspace':0.01, 'hspace':0.01})\n",
    "# for i in range(axes.shape[0]):\n",
    "#     for j in range(axes.shape[1]):\n",
    "#         axes[i, j].imshow(img_plot.pop(), 'gray')\n",
    "#         axes[i, j].set_xticklabels([])\n",
    "#         axes[i, j].set_yticklabels([])\n",
    "# plt.show()"
   ],
   "execution_count": 25,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Model"
   ]
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class _DoubleConvolution(nn.Module):\n",
    "    def __init__(self, in_channels, middle_channel, out_channels, p=0):\n",
    "        super(_DoubleConvolution, self).__init__()\n",
    "        layers = [\n",
    "            nn.Conv2d(in_channels, middle_channel, kernel_size=3, padding=p),\n",
    "            nn.BatchNorm2d(middle_channel),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(middle_channel, out_channels, kernel_size=3, padding=p),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ]\n",
    "        self.encode = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encode(x)\n",
    "\n",
    "\n",
    "class SkullNet(nn.Module):\n",
    "    def __init__(self, num_channels, num_classes):\n",
    "        super(SkullNet, self).__init__()\n",
    "        self.reduce_by = 2\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.C1 = _DoubleConvolution(num_channels, int(64 / self.reduce_by), int(64 / self.reduce_by))\n",
    "        self.C2 = _DoubleConvolution(int(64 / self.reduce_by), int(128 / self.reduce_by), int(128 / self.reduce_by))\n",
    "        self.C3 = _DoubleConvolution(int(128 / self.reduce_by), int(256 / self.reduce_by), int(256 / self.reduce_by))\n",
    "        self.C4 = _DoubleConvolution(int(256 / self.reduce_by), int(512 / self.reduce_by), int(256 / self.reduce_by))\n",
    "        self.C5 = _DoubleConvolution(int(256 / self.reduce_by), int(128 / self.reduce_by), int(128 / self.reduce_by))\n",
    "        self.C6 = _DoubleConvolution(int(128 / self.reduce_by), int(32 / self.reduce_by), 4)\n",
    "        self.fc1 = nn.Linear(4 * 8 * 8, 64)\n",
    "        self.fc2 = nn.Linear(64, 12)\n",
    "\n",
    "    def forward(self, x):\n",
    "        c1 = self.C1(x)\n",
    "        c1_mxp = F.max_pool2d(c1, kernel_size=2, stride=2)\n",
    "\n",
    "        c2 = self.C2(c1_mxp)\n",
    "        c2_mxp = F.max_pool2d(c2, kernel_size=2, stride=2)\n",
    "\n",
    "        c3 = self.C3(c2_mxp)\n",
    "        c3_mxp = F.max_pool2d(c3, kernel_size=2, stride=2)\n",
    "\n",
    "        c4 = self.C4(c3_mxp)\n",
    "        c4_mxp = F.max_pool2d(c4, kernel_size=2, stride=2)\n",
    "\n",
    "        c5 = self.C5(c4_mxp)\n",
    "        c5_mxp = F.max_pool2d(c5, kernel_size=2, stride=2)\n",
    "\n",
    "        c6 = self.C6(c5_mxp)\n",
    "\n",
    "        fc1 = self.fc1(c6.view(-1, 4 * 8 * 8))\n",
    "        fc2 = self.fc2(fc1)\n",
    "        out = fc2.view(fc2.shape[0], 2, -1)\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def match_and_concat(bypass, upsampled, crop=True):\n",
    "        if crop:\n",
    "            c = (bypass.size()[2] - upsampled.size()[2]) // 2\n",
    "            bypass = F.pad(bypass, (-c, -c, -c, -c))\n",
    "        return torch.cat((upsampled, bypass), 1)\n",
    "\n",
    "\n",
    "m = SkullNet(1, 2)\n",
    "torch_total_params = sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
    "print('Total Params:', torch_total_params)"
   ],
   "execution_count": 26,
   "outputs": [
    {
     "output_type": "stream",
     "text": "Total Params: 1016360\n",
     "name": "stdout"
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Train Validation and Test Module"
   ]
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class NNTrainer:\n",
    "\n",
    "    def __init__(self, conf=None, model=None, optimizer=None):\n",
    "\n",
    "        # Initialize parameters and directories before-hand so that we can clearly track which ones are used\n",
    "        self.conf = conf\n",
    "        self.log_dir = self.conf.get('Dirs').get('logs', 'net_logs')\n",
    "        self.epochs = self.conf.get('Params').get('epochs', 100)\n",
    "        self.log_frequency = self.conf.get('Params').get('log_frequency', 10)\n",
    "        self.validation_frequency = self.conf.get('Params').get('validation_frequency', 1)\n",
    "        self.mode = self.conf.get('Params').get('mode', 'test')\n",
    "\n",
    "        # Initialize necessary logging conf\n",
    "        self.checkpoint_file = os.path.join(self.log_dir, self.conf.get('checkpoint_file'))\n",
    "\n",
    "        self.log_headers = self.get_log_headers()\n",
    "        _log_key = self.conf.get('checkpoint_file').split('.')[0]\n",
    "        self.test_logger = NNTrainer.get_logger(log_file=os.path.join(self.log_dir, 'submission.csv'),\n",
    "                                                header=self.log_headers.get('test', ''))\n",
    "        if self.mode == 'train':\n",
    "            self.train_logger = NNTrainer.get_logger(log_file=os.path.join(self.log_dir, _log_key + '-TRAIN.csv'),\n",
    "                                                     header=self.log_headers.get('train', ''))\n",
    "            self.val_logger = NNTrainer.get_logger(log_file=os.path.join(self.log_dir, _log_key + '-VAL.csv'),\n",
    "                                                   header=self.log_headers.get('validation', ''))\n",
    "\n",
    "        self.f_dyn_weights = self.conf.get('f_dyn_weights')\n",
    "\n",
    "        # Handle gpu/cpu\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.device(\"cuda\" if self.conf['Params'].get('use_gpu', False) else \"cpu\")\n",
    "        else:\n",
    "            print('### GPU not found.')\n",
    "            self.device = torch.device(\"cpu\")\n",
    "\n",
    "        # Initialization to save model\n",
    "        self.model = model.to(self.device)\n",
    "        self.optimizer = optimizer\n",
    "        self.checkpoint = {'total_epochs:': 0, 'epochs': 0, 'state': None, 'score': 0.0, 'model': 'EMPTY'}\n",
    "        self.patience = self.conf.get('Params').get('patience', 35)\n",
    "\n",
    "    def test(self, data_loaders=None):\n",
    "        print('------Running test------')\n",
    "        score = ScoreAccumulator()\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(data_loaders, 1):\n",
    "                inputs, labels = data['inputs'].to(self.device).float(), data['labels'].to(self.device).long()\n",
    "\n",
    "                if self.model.training:\n",
    "                    self.optimizer.zero_grad()\n",
    "\n",
    "                outputs = self.model(inputs)\n",
    "                for ix, pred  in zip(data['index'], F.softmax(outputs, 1)[:,1,:]):\n",
    "                    file = data_loaders.dataset.indices[ix][0].split('.')[0]\n",
    "\n",
    "                    p_EPIDURAL = pred[0].item()\n",
    "                    p_INTRAPARENCHYMAL = pred[1].item()\n",
    "                    p_INTRAVENTRICULAR = pred[2].item()\n",
    "                    p_SUBARACHNOID = pred[3].item()\n",
    "                    p_SUBDURAL = pred[4].item()\n",
    "                    p_ANY = pred[5].item()\n",
    "\n",
    "                    log = file + '_' + EPIDURAL + ',' + str(p_EPIDURAL)\n",
    "                    log += '\\n' + file + '_' + INTRAPARENCHYMAL + ',' + str(p_INTRAPARENCHYMAL)\n",
    "                    log += '\\n' + file + '_' + INTRAVENTRICULAR + ',' + str(p_INTRAVENTRICULAR)\n",
    "                    log += '\\n' + file + '_' + SUBARACHNOID + ',' + str(p_SUBARACHNOID)\n",
    "                    log += '\\n' + file + '_' + SUBDURAL + ',' + str(p_SUBDURAL) \n",
    "                    log += '\\n' + file + '_' + ANY + ',' + str(p_ANY)\n",
    "                    print(file, end='\\r')\n",
    "                    NNTrainer.flush(self.test_logger, log)\n",
    "\n",
    "        self._on_test_end(log_file=self.test_logger.name)\n",
    "        if not self.test_logger and not self.test_logger.closed:\n",
    "            self.test_logger.close()\n",
    "\n",
    "    def train(self, data_loader=None, validation_loader=None, epoch_run=None):\n",
    "        print('Training...')\n",
    "        for epoch in range(1, self.epochs + 1):\n",
    "            self.model.train()\n",
    "            self._adjust_learning_rate(epoch=epoch)\n",
    "            self.checkpoint['total_epochs'] = epoch\n",
    "\n",
    "            # Run one epoch\n",
    "            epoch_run(epoch=epoch, data_loader=data_loader, logger=self.train_logger)\n",
    "\n",
    "            self._on_epoch_end(data_loader=data_loader, log_file=self.train_logger.name)\n",
    "\n",
    "            # Validation_frequency is the number of epoch until validation\n",
    "            if epoch % self.validation_frequency == 0:\n",
    "                print('############# Running validation... ####################')\n",
    "                self.model.eval()\n",
    "                with torch.no_grad():\n",
    "                    self.validation(epoch=epoch, validation_loader=validation_loader, epoch_run=epoch_run)\n",
    "                self._on_validation_end(data_loader=validation_loader, log_file=self.val_logger.name)\n",
    "                if self.early_stop(patience=self.patience):\n",
    "                    return\n",
    "                print('########################################################')\n",
    "\n",
    "        if not self.train_logger and not self.train_logger.closed:\n",
    "            self.train_logger.close()\n",
    "        if not self.val_logger and not self.val_logger.closed:\n",
    "            self.val_logger.close()\n",
    "\n",
    "    def _on_epoch_end(self, **kw):\n",
    "        viz.plot_column_keys(file=kw['log_file'], batches_per_epoch=kw['data_loader'].__len__(),\n",
    "                              keys=['F1', 'LOSS', 'ACCURACY'])\n",
    "        viz.plot_cmap(file=kw['log_file'], save=True, x='PRECISION', y='RECALL')\n",
    "\n",
    "    def _on_validation_end(self, **kw):\n",
    "        viz.plot_column_keys(file=kw['log_file'], batches_per_epoch=kw['data_loader'].__len__(),\n",
    "                              keys=['F1', 'ACCURACY'])\n",
    "        viz.plot_cmap(file=kw['log_file'], save=True, x='PRECISION', y='RECALL')\n",
    "\n",
    "    def _on_test_end(self, **kw):\n",
    "        viz.y_scatter(file=kw['log_file'], y='F1', label='ID', save=True, title='Test')\n",
    "        viz.y_scatter(file=kw['log_file'], y='ACCURACY', label='ID', save=True, title='Test')\n",
    "        viz.xy_scatter(file=kw['log_file'], save=True, x='PRECISION', y='RECALL', label='ID', title='Test')\n",
    "\n",
    "    # Headers for log files\n",
    "    def get_log_headers(self):\n",
    "        return {\n",
    "            'train': 'ID,EPOCH,BATCH,PRECISION,RECALL,F1,ACCURACY,LOSS',\n",
    "            'validation': 'ID,PRECISION,RECALL,F1,ACCURACY',\n",
    "            'test': 'ID,Label'\n",
    "        }\n",
    "    \n",
    "    def validation(self, epoch=None, validation_loader=None, epoch_run=None):\n",
    "        score_acc = ScoreAccumulator()\n",
    "        epoch_run(epoch=epoch, data_loader=validation_loader, logger=self.val_logger, score_acc=score_acc)\n",
    "        p, r, f1, a = score_acc.get_prfa()\n",
    "        print('>>> PRF1: ', [p, r, f1, a])\n",
    "        self._save_if_better(score=f1)\n",
    "\n",
    "    def resume_from_checkpoint(self, parallel_trained=False):\n",
    "        self.checkpoint = torch.load(self.checkpoint_file)\n",
    "        print(self.checkpoint_file, 'Loaded...')\n",
    "        try:\n",
    "            if parallel_trained:\n",
    "                from collections import OrderedDict\n",
    "                new_state_dict = OrderedDict()\n",
    "                for k, v in self.checkpoint['state'].items():\n",
    "                    name = k[7:]  # remove `module.`\n",
    "                    new_state_dict[name] = v\n",
    "                # load params\n",
    "                self.model.load_state_dict(new_state_dict)\n",
    "            else:\n",
    "                self.model.load_state_dict(self.checkpoint['state'])\n",
    "        except Exception as e:\n",
    "            print('ERROR: ' + str(e))\n",
    "\n",
    "    def _save_if_better(self, score=None):\n",
    "\n",
    "        if self.mode == 'test':\n",
    "            return\n",
    "\n",
    "        if score > self.checkpoint['score']:\n",
    "            print('Score improved: ',\n",
    "                  str(self.checkpoint['score']) + ' to ' + str(score) + ' BEST CHECKPOINT SAVED')\n",
    "            self.checkpoint['state'] = self.model.state_dict()\n",
    "            self.checkpoint['epochs'] = self.checkpoint['total_epochs']\n",
    "            self.checkpoint['score'] = score\n",
    "            self.checkpoint['model'] = str(self.model)\n",
    "            torch.save(self.checkpoint, self.checkpoint_file)\n",
    "        else:\n",
    "            print('Score did not improve:' + str(score) + ' BEST: ' + str(self.checkpoint['score']) + ' Best EP: ' + (\n",
    "                str(self.checkpoint['epochs'])))\n",
    "\n",
    "    def early_stop(self, patience=35):\n",
    "        return self.checkpoint['total_epochs'] - self.checkpoint['epochs'] >= patience * self.validation_frequency\n",
    "\n",
    "    @staticmethod\n",
    "    def get_logger(log_file=None, header=''):\n",
    "\n",
    "        if os.path.isfile(log_file):\n",
    "            print('### CRITICAL!!! ' + log_file + '\" already exists.')\n",
    "            ip = input('Override? [Y/N]: ')\n",
    "            if ip == 'N' or ip == 'n':\n",
    "                sys.exit(1)\n",
    "\n",
    "        file = open(log_file, 'w')\n",
    "        NNTrainer.flush(file, header)\n",
    "        return file\n",
    "\n",
    "    @staticmethod\n",
    "    def flush(logger, msg):\n",
    "        if logger is not None:\n",
    "            logger.write(msg + '\\n')\n",
    "            logger.flush()\n",
    "\n",
    "    def _adjust_learning_rate(self, epoch):\n",
    "        if epoch % 30 == 0:\n",
    "            for param_group in self.optimizer.param_groups:\n",
    "                if param_group['lr'] >= 1e-5:\n",
    "                    param_group['lr'] = param_group['lr'] * 0.7\n",
    "        \n",
    "    def epoch_ce_loss(self, **kw):\n",
    "        \"\"\"\n",
    "        One epoch implementation of binary cross-entropy loss\n",
    "        :param kw:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        running_loss = 0.0\n",
    "        score_acc = ScoreAccumulator() if self.model.training else kw.get('score_acc')\n",
    "        assert isinstance(score_acc, ScoreAccumulator)\n",
    "\n",
    "        for i, data in enumerate(kw['data_loader'], 1):\n",
    "            inputs, labels = data['inputs'].to(self.device).float(), data['labels'].to(self.device).long()\n",
    "            \n",
    "            if self.model.training:\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "            outputs = self.model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            loss = F.nll_loss(F.log_softmax(outputs, 1), labels, weight=torch.FloatTensor(self.f_dyn_weights(self.conf)).to(self.device))\n",
    "            \n",
    "            if self.model.training:\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            current_loss = loss.item()\n",
    "            running_loss += current_loss\n",
    "\n",
    "            if self.model.training:\n",
    "                score_acc.reset()\n",
    "\n",
    "            p, r, f1, a = score_acc.add_tensor(predicted, labels).get_prfa()\n",
    "\n",
    "            if i % self.log_frequency == 0:\n",
    "                print('Epochs[%d/%d] Batch[%d/%d] loss:%.5f pre:%.3f rec:%.3f f1:%.3f acc:%.3f' %\n",
    "                      (\n",
    "                          kw['epoch'], self.epochs, i, kw['data_loader'].__len__(),\n",
    "                          running_loss / self.log_frequency, p, r, f1,\n",
    "                          a))\n",
    "                running_loss = 0.0\n",
    "            self.flush(kw['logger'],\n",
    "                       ','.join(str(x) for x in [0, kw['epoch'], i, p, r, f1, a, current_loss]))"
   ],
   "execution_count": 27,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training setup"
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "\"\"\"\n### author: Aashis Khanal\n### sraashis@gmail.com\n### date: 9/10/2018\n\"\"\"\n\nimport os\nimport traceback\n\nimport torch\nimport torch.optim as optim\n\n\ndef run(runs):\n    for R in runs:\n        for k, folder in R['Dirs'].items():\n            os.makedirs(folder, exist_ok=True)\n        R['acc'] = ScoreAccumulator()\n        R['checkpoint_file'] = R['train_mapping_file'].split(os.sep)[1] + '.tar'\n        model = SkullNet(R['Params']['num_channels'], R['Params']['num_classes'])\n        optimizer = optim.Adam(model.parameters(), lr=R['Params']['learning_rate'])\n        if R['Params']['distribute']:\n            model = torch.nn.DataParallel(model)\n            model.float()\n            optimizer = optim.Adam(model.module.parameters(), lr=R['Params']['learning_rate'])\n\n        try:\n            trainer = NNTrainer(model=model, conf=R, optimizer=optimizer)\n            if R.get('Params').get('mode') == 'train':\n                train_loader, val_loader = SkullDataset.get_train_val_loader(R)\n                print('### Train Val Batch size:', len(train_loader.dataset), len(val_loader.dataset))\n                trainer.train(data_loader=train_loader, validation_loader=val_loader,\n                              epoch_run=trainer.epoch_ce_loss)\n\n            test_loader = SkullDataset.get_test_loader(conf=R)\n            trainer.resume_from_checkpoint(parallel_trained=R.get('Params').get('parallel_trained'))\n            \n            trainer.test(test_loader)\n        except Exception as e:\n            traceback.print_exc()\n\n    print(R['acc'].get_prfa())\n    f = open(R['Dirs']['logs'] + os.sep + 'score.txt', \"w\")\n    f.write(', '.join(str(s) for s in R['acc'].get_prfa()))\n    f.close()\n",
   "execution_count": 28,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "Params = {\n    'num_channels': 1,\n    'num_classes': 2,\n    'batch_size': 16,\n    'epochs': 1,\n    'learning_rate': 0.001,\n    'use_gpu': True,\n    'distribute': True,\n    'shuffle': True,\n    'log_frequency': 10,\n    'validation_frequency': 1,\n    'mode': 'train',\n    'parallel_trained': False,\n}\nSKDB = {\n    'Params': Params,\n    'Dirs': {\n        'train_image_dir': train_images_dir,\n        'test_image_dir': test_images_dir,\n        'logs': 'logs'},\n    'train_mapping_file': train_mapping_file,\n    'test_mapping_file': test_mapping_file,\n    'f_dyn_weights': lambda x: np.random.choice(np.arange(1, 101, 1), 2),\n    'validate_img_pth': False,\n    'load_lim': 1000\n}",
   "execution_count": 29,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true,
    "scrolled": false,
    "_kg_hide-output": true
   },
   "cell_type": "code",
   "source": "run([SKDB])",
   "execution_count": 30,
   "outputs": [
    {
     "output_type": "stream",
     "text": "### CRITICAL!!! logs/submission.csv\" already exists.\nOverride? [Y/N]: \n### CRITICAL!!! logs/input-TRAIN.csv\" already exists.\nOverride? [Y/N]: \n### CRITICAL!!! logs/input-VAL.csv\" already exists.\nOverride? [Y/N]: \n### Train Val Batch size: 800 200\nTraining...\nEpochs[1/1] Batch[10/50] loss:0.50157 pre:0.001 rec:0.001 f1:0.001 acc:0.927\nEpochs[1/1] Batch[20/50] loss:0.18213 pre:0.001 rec:0.001 f1:0.001 acc:0.802\nEpochs[1/1] Batch[30/50] loss:0.37367 pre:0.001 rec:0.001 f1:0.001 acc:0.938\n",
     "name": "stdout"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-3cb620fbbfbd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mSKDB\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-28-539b2c8578d2>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(runs)\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'### Train Val Batch size:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                 trainer.train(data_loader=train_loader, validation_loader=val_loader,\n\u001b[0;32m---> 33\u001b[0;31m                               epoch_run=trainer.epoch_ce_loss)\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSkullDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_test_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-563094911230>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data_loader, validation_loader, epoch_run)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;31m# Run one epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0mepoch_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_logger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_on_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-563094911230>\u001b[0m in \u001b[0;36mepoch_ce_loss\u001b[0;34m(self, **kw)\u001b[0m\n\u001b[1;32m    228\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m             \u001b[0mcurrent_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m             \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcurrent_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ]
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "version": "3.6.4",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python",
   "mimetype": "text/x-python"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
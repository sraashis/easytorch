{
 "cells": [
  {
   "metadata": {
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "import math\n",
    "import os\n",
    "from itertools import islice\n",
    "sep = os.sep\n",
    "import numpy as np\n",
    "import pydicom\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data.dataset import Dataset, random_split\n",
    "import torchvision.transforms as tmf"
   ],
   "execution_count": 7,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "import img_utils as iu\n",
    "import nnviz as viz\n",
    "from measurements import ScoreAccumulator"
   ],
   "execution_count": 8,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "EPIDURAL = 'epidural'\n",
    "INTRAPARENCHYMAL = 'intraparenchymal'\n",
    "INTRAVENTRICULAR = 'intraventricular'\n",
    "SUBARACHNOID = 'subarachnoid'\n",
    "SUBDURAL = 'subdural'\n",
    "ANY = 'any'"
   ],
   "execution_count": 9,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "train_mapping_file = '../input/rsna-intracranial-hemorrhage-detection/stage_1_train.csv'\n",
    "train_images_dir = '../input/rsna-intracranial-hemorrhage-detection/stage_1_train_images/'\n",
    "test_mapping_file = '../input/rsna-intracranial-hemorrhage-detection/stage_1_sample_submission.csv'\n",
    "test_images_dir = '../input/rsna-intracranial-hemorrhage-detection/stage_1_test_images/'"
   ],
   "execution_count": 10,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "train_transforms = tmf.Compose([\n",
    "    tmf.ToPILImage(),\n",
    "    tmf.Resize((512, 512), interpolation=2),\n",
    "    tmf.RandomHorizontalFlip(),\n",
    "    tmf.RandomVerticalFlip(),\n",
    "    tmf.ToTensor()\n",
    "])"
   ],
   "execution_count": 21,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "test_transforms = tmf.Compose([\n",
    "    tmf.ToPILImage(),\n",
    "    tmf.Resize((512, 512), interpolation=2),\n",
    "    tmf.ToTensor()\n",
    "])"
   ],
   "execution_count": 22,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "class SkullDataset(Dataset):\n",
    "    def __init__(self, conf=None, mode=None, transforms=None):\n",
    "        self.transforms = transforms\n",
    "        self.mode = mode\n",
    "        self.conf = conf\n",
    "        self.image_dir = None\n",
    "        self.mapping_file = None\n",
    "        self.expand_by = self.conf.get('Params').get('expand_patch_by')\n",
    "        self.indices = []\n",
    "        self.LIM = conf.get('load_lim', 10e10)\n",
    "\n",
    "    def load_data_indices(self, validate_pth=False):\n",
    "        with open(self.mapping_file) as infile:\n",
    "            linecount, six_rows, _ = 1, True, next(infile)\n",
    "            while six_rows:\n",
    "                print('Reading Line: {}'.format(linecount), end='\\r')\n",
    "\n",
    "                six_rows = list(r.rstrip().split(',') for r in islice(infile, 6))\n",
    "                image_file, cat_label = None, []\n",
    "                for hname, label in six_rows:\n",
    "                    (ID, file_ID, htype), label = hname.split('_'), int(label if self.mode == 'train' else 0)\n",
    "                    fname_ = ID + '_' + file_ID + '.dcm'\n",
    "\n",
    "                    if validate_pth and not os.path.exists(os.path.join(self.image_dir, fname_)):\n",
    "                        break\n",
    "\n",
    "                    if image_file and fname_ != image_file:\n",
    "                        print('Mismatch Line: {}'.format(linecount), end='\\r')\n",
    "                        break\n",
    "                    else:\n",
    "                        image_file = fname_\n",
    "\n",
    "                    cat_label.append(label)\n",
    "\n",
    "                if image_file and len(cat_label) == 6:\n",
    "                    self.indices.append([image_file, np.array(cat_label)])\n",
    "                    if len(self) >= self.LIM:\n",
    "                        break\n",
    "                linecount += 6\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_file, label = self.indices[index]\n",
    "\n",
    "        dcm = pydicom.dcmread(self.image_dir + os.sep + image_file)\n",
    "        img_arr = np.array(iu.rescale2d_unsigned(dcm.pixel_array) * 255, dtype=np.uint8)\n",
    "        img_arr = iu.apply_clahe(img_arr)\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img_arr = self.transforms(img_arr)\n",
    "            \n",
    "        return {'inputs':img_arr, 'labels':label, 'index':index}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    @classmethod\n",
    "    def get_test_loader(cls, conf, shuffle_indices=False):\n",
    "        testset = cls(conf, 'test', test_transforms)\n",
    "        testset.image_dir = conf['Dirs']['test_image_dir']\n",
    "        testset.mapping_file = conf['test_mapping_file']\n",
    "        testset.load_data_indices(conf['validate_img_pth'])\n",
    "        return DataLoader(dataset=testset, batch_size=conf['Params']['batch_size'], shuffle=shuffle_indices,\n",
    "                          num_workers=5, drop_last=False)\n",
    "\n",
    "    @classmethod\n",
    "    def get_train_val_loader(cls, conf, shuffle_indices=True, drop_last_batch=True, split_ratio=[0.8, 0.2]):\n",
    "        full_dataset = cls(conf, 'train', train_transforms)\n",
    "        full_dataset.image_dir = conf['Dirs']['train_image_dir']\n",
    "        full_dataset.mapping_file = conf['train_mapping_file']\n",
    "        full_dataset.load_data_indices(conf['validate_img_pth'])\n",
    "        size_a = math.ceil(split_ratio[0] * len(full_dataset))\n",
    "        size_b = math.floor(split_ratio[1] * len(full_dataset))\n",
    "        dataset_a, dataset_b = random_split(full_dataset, [size_a, size_b])\n",
    "        loader_a = DataLoader(dataset_a,\n",
    "                              batch_size=conf['Params']['batch_size'], \n",
    "                              shuffle=shuffle_indices, num_workers=3, \n",
    "                              drop_last=drop_last_batch)\n",
    "        loader_b = DataLoader(dataset_b,\n",
    "                              batch_size=conf['Params']['batch_size'],\n",
    "                              shuffle=shuffle_indices, num_workers=3, \n",
    "                              drop_last=drop_last_batch)\n",
    "        return loader_a, loader_b"
   ],
   "execution_count": 24,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "# img_plot = images_arr.copy()\n",
    "# plt.tight_layout()\n",
    "# fig, axes = plt.subplots(4, 3, figsize=(10, 18), gridspec_kw = {'wspace':0.01, 'hspace':0.01})\n",
    "# for i in range(axes.shape[0]):\n",
    "#     for j in range(axes.shape[1]):\n",
    "#         axes[i, j].imshow(img_plot.pop(), 'gray')\n",
    "#         axes[i, j].set_xticklabels([])\n",
    "#         axes[i, j].set_yticklabels([])\n",
    "# plt.show()"
   ],
   "execution_count": 25,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Model"
   ]
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class _DoubleConvolution(nn.Module):\n",
    "    def __init__(self, in_channels, middle_channel, out_channels, p=0):\n",
    "        super(_DoubleConvolution, self).__init__()\n",
    "        layers = [\n",
    "            nn.Conv2d(in_channels, middle_channel, kernel_size=3, padding=p),\n",
    "            nn.BatchNorm2d(middle_channel),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(middle_channel, out_channels, kernel_size=3, padding=p),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ]\n",
    "        self.encode = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encode(x)\n",
    "\n",
    "\n",
    "class SkullNet(nn.Module):\n",
    "    def __init__(self, num_channels, num_classes):\n",
    "        super(SkullNet, self).__init__()\n",
    "        self.reduce_by = 2\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.C1 = _DoubleConvolution(num_channels, int(64 / self.reduce_by), int(64 / self.reduce_by))\n",
    "        self.C2 = _DoubleConvolution(int(64 / self.reduce_by), int(128 / self.reduce_by), int(128 / self.reduce_by))\n",
    "        self.C3 = _DoubleConvolution(int(128 / self.reduce_by), int(256 / self.reduce_by), int(256 / self.reduce_by))\n",
    "        self.C4 = _DoubleConvolution(int(256 / self.reduce_by), int(512 / self.reduce_by), int(256 / self.reduce_by))\n",
    "        self.C5 = _DoubleConvolution(int(256 / self.reduce_by), int(128 / self.reduce_by), int(128 / self.reduce_by))\n",
    "        self.C6 = _DoubleConvolution(int(128 / self.reduce_by), int(32 / self.reduce_by), 4)\n",
    "        self.fc1 = nn.Linear(4 * 8 * 8, 64)\n",
    "        self.fc2 = nn.Linear(64, 12)\n",
    "\n",
    "    def forward(self, x):\n",
    "        c1 = self.C1(x)\n",
    "        c1_mxp = F.max_pool2d(c1, kernel_size=2, stride=2)\n",
    "\n",
    "        c2 = self.C2(c1_mxp)\n",
    "        c2_mxp = F.max_pool2d(c2, kernel_size=2, stride=2)\n",
    "\n",
    "        c3 = self.C3(c2_mxp)\n",
    "        c3_mxp = F.max_pool2d(c3, kernel_size=2, stride=2)\n",
    "\n",
    "        c4 = self.C4(c3_mxp)\n",
    "        c4_mxp = F.max_pool2d(c4, kernel_size=2, stride=2)\n",
    "\n",
    "        c5 = self.C5(c4_mxp)\n",
    "        c5_mxp = F.max_pool2d(c5, kernel_size=2, stride=2)\n",
    "\n",
    "        c6 = self.C6(c5_mxp)\n",
    "\n",
    "        fc1 = self.fc1(c6.view(-1, 4 * 8 * 8))\n",
    "        fc2 = self.fc2(fc1)\n",
    "        out = fc2.view(fc2.shape[0], 2, -1)\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def match_and_concat(bypass, upsampled, crop=True):\n",
    "        if crop:\n",
    "            c = (bypass.size()[2] - upsampled.size()[2]) // 2\n",
    "            bypass = F.pad(bypass, (-c, -c, -c, -c))\n",
    "        return torch.cat((upsampled, bypass), 1)\n",
    "\n",
    "\n",
    "m = SkullNet(1, 2)\n",
    "torch_total_params = sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
    "print('Total Params:', torch_total_params)"
   ],
   "execution_count": 26,
   "outputs": [
    {
     "output_type": "stream",
     "text": "Total Params: 1016360\n",
     "name": "stdout"
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Train Validation and Test Module"
   ]
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class NNTrainer:\n",
    "\n",
    "    def __init__(self, conf=None, model=None, optimizer=None):\n",
    "\n",
    "        # Initialize parameters and directories before-hand so that we can clearly track which ones are used\n",
    "        self.conf = conf\n",
    "        self.log_dir = self.conf.get('Dirs').get('logs', 'net_logs')\n",
    "        self.epochs = self.conf.get('Params').get('epochs', 100)\n",
    "        self.log_frequency = self.conf.get('Params').get('log_frequency', 10)\n",
    "        self.validation_frequency = self.conf.get('Params').get('validation_frequency', 1)\n",
    "        self.mode = self.conf.get('Params').get('mode', 'test')\n",
    "\n",
    "        # Initialize necessary logging conf\n",
    "        self.checkpoint_file = os.path.join(self.log_dir, self.conf.get('checkpoint_file'))\n",
    "\n",
    "        self.log_headers = self.get_log_headers()\n",
    "        _log_key = self.conf.get('checkpoint_file').split('.')[0]\n",
    "        self.test_logger = NNTrainer.get_logger(log_file=os.path.join(self.log_dir, 'submission.csv'),\n",
    "                                                header=self.log_headers.get('test', ''))\n",
    "        if self.mode == 'train':\n",
    "            self.train_logger = NNTrainer.get_logger(log_file=os.path.join(self.log_dir, _log_key + '-TRAIN.csv'),\n",
    "                                                     header=self.log_headers.get('train', ''))\n",
    "            self.val_logger = NNTrainer.get_logger(log_file=os.path.join(self.log_dir, _log_key + '-VAL.csv'),\n",
    "                                                   header=self.log_headers.get('validation', ''))\n",
    "\n",
    "        self.f_dyn_weights = self.conf.get('f_dyn_weights')\n",
    "\n",
    "        # Handle gpu/cpu\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.device(\"cuda\" if self.conf['Params'].get('use_gpu', False) else \"cpu\")\n",
    "        else:\n",
    "            print('### GPU not found.')\n",
    "            self.device = torch.device(\"cpu\")\n",
    "\n",
    "        # Initialization to save model\n",
    "        self.model = model.to(self.device)\n",
    "        self.optimizer = optimizer\n",
    "        self.checkpoint = {'total_epochs:': 0, 'epochs': 0, 'state': None, 'score': 0.0, 'model': 'EMPTY'}\n",
    "        self.patience = self.conf.get('Params').get('patience', 35)\n",
    "\n",
    "    def test(self, data_loaders=None):\n",
    "        print('------Running test------')\n",
    "        score = ScoreAccumulator()\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(data_loaders, 1):\n",
    "                inputs, labels = data['inputs'].to(self.device).float(), data['labels'].to(self.device).long()\n",
    "\n",
    "                if self.model.training:\n",
    "                    self.optimizer.zero_grad()\n",
    "\n",
    "                outputs = self.model(inputs)\n",
    "                for ix, pred  in zip(data['index'], F.softmax(outputs, 1)[:,1,:]):\n",
    "                    file = data_loaders.dataset.indices[ix][0].split('.')[0]\n",
    "\n",
    "                    p_EPIDURAL = pred[0].item()\n",
    "                    p_INTRAPARENCHYMAL = pred[1].item()\n",
    "                    p_INTRAVENTRICULAR = pred[2].item()\n",
    "                    p_SUBARACHNOID = pred[3].item()\n",
    "                    p_SUBDURAL = pred[4].item()\n",
    "                    p_ANY = pred[5].item()\n",
    "\n",
    "                    log = file + '_' + EPIDURAL + ',' + str(p_EPIDURAL)\n",
    "                    log += '\\n' + file + '_' + INTRAPARENCHYMAL + ',' + str(p_INTRAPARENCHYMAL)\n",
    "                    log += '\\n' + file + '_' + INTRAVENTRICULAR + ',' + str(p_INTRAVENTRICULAR)\n",
    "                    log += '\\n' + file + '_' + SUBARACHNOID + ',' + str(p_SUBARACHNOID)\n",
    "                    log += '\\n' + file + '_' + SUBDURAL + ',' + str(p_SUBDURAL) \n",
    "                    log += '\\n' + file + '_' + ANY + ',' + str(p_ANY)\n",
    "                    print(file, end='\\r')\n",
    "                    NNTrainer.flush(self.test_logger, log)\n",
    "\n",
    "        self._on_test_end(log_file=self.test_logger.name)\n",
    "        if not self.test_logger and not self.test_logger.closed:\n",
    "            self.test_logger.close()\n",
    "\n",
    "    def train(self, data_loader=None, validation_loader=None, epoch_run=None):\n",
    "        print('Training...')\n",
    "        for epoch in range(1, self.epochs + 1):\n",
    "            self.model.train()\n",
    "            self._adjust_learning_rate(epoch=epoch)\n",
    "            self.checkpoint['total_epochs'] = epoch\n",
    "\n",
    "            # Run one epoch\n",
    "            epoch_run(epoch=epoch, data_loader=data_loader, logger=self.train_logger)\n",
    "\n",
    "            self._on_epoch_end(data_loader=data_loader, log_file=self.train_logger.name)\n",
    "\n",
    "            # Validation_frequency is the number of epoch until validation\n",
    "            if epoch % self.validation_frequency == 0:\n",
    "                print('############# Running validation... ####################')\n",
    "                self.model.eval()\n",
    "                with torch.no_grad():\n",
    "                    self.validation(epoch=epoch, validation_loader=validation_loader, epoch_run=epoch_run)\n",
    "                self._on_validation_end(data_loader=validation_loader, log_file=self.val_logger.name)\n",
    "                if self.early_stop(patience=self.patience):\n",
    "                    return\n",
    "                print('########################################################')\n",
    "\n",
    "        if not self.train_logger and not self.train_logger.closed:\n",
    "            self.train_logger.close()\n",
    "        if not self.val_logger and not self.val_logger.closed:\n",
    "            self.val_logger.close()\n",
    "\n",
    "    def _on_epoch_end(self, **kw):\n",
    "        viz.plot_column_keys(file=kw['log_file'], batches_per_epoch=kw['data_loader'].__len__(),\n",
    "                              keys=['F1', 'LOSS', 'ACCURACY'])\n",
    "        viz.plot_cmap(file=kw['log_file'], save=True, x='PRECISION', y='RECALL')\n",
    "\n",
    "    def _on_validation_end(self, **kw):\n",
    "        viz.plot_column_keys(file=kw['log_file'], batches_per_epoch=kw['data_loader'].__len__(),\n",
    "                              keys=['F1', 'ACCURACY'])\n",
    "        viz.plot_cmap(file=kw['log_file'], save=True, x='PRECISION', y='RECALL')\n",
    "\n",
    "    def _on_test_end(self, **kw):\n",
    "        viz.y_scatter(file=kw['log_file'], y='F1', label='ID', save=True, title='Test')\n",
    "        viz.y_scatter(file=kw['log_file'], y='ACCURACY', label='ID', save=True, title='Test')\n",
    "        viz.xy_scatter(file=kw['log_file'], save=True, x='PRECISION', y='RECALL', label='ID', title='Test')\n",
    "\n",
    "    # Headers for log files\n",
    "    def get_log_headers(self):\n",
    "        return {\n",
    "            'train': 'ID,EPOCH,BATCH,PRECISION,RECALL,F1,ACCURACY,LOSS',\n",
    "            'validation': 'ID,PRECISION,RECALL,F1,ACCURACY',\n",
    "            'test': 'ID,Label'\n",
    "        }\n",
    "    \n",
    "    def validation(self, epoch=None, validation_loader=None, epoch_run=None):\n",
    "        score_acc = ScoreAccumulator()\n",
    "        epoch_run(epoch=epoch, data_loader=validation_loader, logger=self.val_logger, score_acc=score_acc)\n",
    "        p, r, f1, a = score_acc.get_prfa()\n",
    "        print('>>> PRF1: ', [p, r, f1, a])\n",
    "        self._save_if_better(score=f1)\n",
    "\n",
    "    def resume_from_checkpoint(self, parallel_trained=False):\n",
    "        self.checkpoint = torch.load(self.checkpoint_file)\n",
    "        print(self.checkpoint_file, 'Loaded...')\n",
    "        try:\n",
    "            if parallel_trained:\n",
    "                from collections import OrderedDict\n",
    "                new_state_dict = OrderedDict()\n",
    "                for k, v in self.checkpoint['state'].items():\n",
    "                    name = k[7:]  # remove `module.`\n",
    "                    new_state_dict[name] = v\n",
    "                # load params\n",
    "                self.model.load_state_dict(new_state_dict)\n",
    "            else:\n",
    "                self.model.load_state_dict(self.checkpoint['state'])\n",
    "        except Exception as e:\n",
    "            print('ERROR: ' + str(e))\n",
    "\n",
    "    def _save_if_better(self, score=None):\n",
    "\n",
    "        if self.mode == 'test':\n",
    "            return\n",
    "\n",
    "        if score > self.checkpoint['score']:\n",
    "            print('Score improved: ',\n",
    "                  str(self.checkpoint['score']) + ' to ' + str(score) + ' BEST CHECKPOINT SAVED')\n",
    "            self.checkpoint['state'] = self.model.state_dict()\n",
    "            self.checkpoint['epochs'] = self.checkpoint['total_epochs']\n",
    "            self.checkpoint['score'] = score\n",
    "            self.checkpoint['model'] = str(self.model)\n",
    "            torch.save(self.checkpoint, self.checkpoint_file)\n",
    "        else:\n",
    "            print('Score did not improve:' + str(score) + ' BEST: ' + str(self.checkpoint['score']) + ' Best EP: ' + (\n",
    "                str(self.checkpoint['epochs'])))\n",
    "\n",
    "    def early_stop(self, patience=35):\n",
    "        return self.checkpoint['total_epochs'] - self.checkpoint['epochs'] >= patience * self.validation_frequency\n",
    "\n",
    "    @staticmethod\n",
    "    def get_logger(log_file=None, header=''):\n",
    "\n",
    "        if os.path.isfile(log_file):\n",
    "            print('### CRITICAL!!! ' + log_file + '\" already exists.')\n",
    "            ip = input('Override? [Y/N]: ')\n",
    "            if ip == 'N' or ip == 'n':\n",
    "                sys.exit(1)\n",
    "\n",
    "        file = open(log_file, 'w')\n",
    "        NNTrainer.flush(file, header)\n",
    "        return file\n",
    "\n",
    "    @staticmethod\n",
    "    def flush(logger, msg):\n",
    "        if logger is not None:\n",
    "            logger.write(msg + '\\n')\n",
    "            logger.flush()\n",
    "\n",
    "    def _adjust_learning_rate(self, epoch):\n",
    "        if epoch % 30 == 0:\n",
    "            for param_group in self.optimizer.param_groups:\n",
    "                if param_group['lr'] >= 1e-5:\n",
    "                    param_group['lr'] = param_group['lr'] * 0.7\n",
    "        \n",
    "    def epoch_ce_loss(self, **kw):\n",
    "        \"\"\"\n",
    "        One epoch implementation of binary cross-entropy loss\n",
    "        :param kw:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        running_loss = 0.0\n",
    "        score_acc = ScoreAccumulator() if self.model.training else kw.get('score_acc')\n",
    "        assert isinstance(score_acc, ScoreAccumulator)\n",
    "\n",
    "        for i, data in enumerate(kw['data_loader'], 1):\n",
    "            inputs, labels = data['inputs'].to(self.device).float(), data['labels'].to(self.device).long()\n",
    "            \n",
    "            if self.model.training:\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "            outputs = self.model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            loss = F.nll_loss(F.log_softmax(outputs, 1), labels, weight=torch.FloatTensor(self.f_dyn_weights(self.conf)).to(self.device))\n",
    "            \n",
    "            if self.model.training:\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            current_loss = loss.item()\n",
    "            running_loss += current_loss\n",
    "\n",
    "            if self.model.training:\n",
    "                score_acc.reset()\n",
    "\n",
    "            p, r, f1, a = score_acc.add_tensor(predicted, labels).get_prfa()\n",
    "\n",
    "            if i % self.log_frequency == 0:\n",
    "                print('Epochs[%d/%d] Batch[%d/%d] loss:%.5f pre:%.3f rec:%.3f f1:%.3f acc:%.3f' %\n",
    "                      (\n",
    "                          kw['epoch'], self.epochs, i, kw['data_loader'].__len__(),\n",
    "                          running_loss / self.log_frequency, p, r, f1,\n",
    "                          a))\n",
    "                running_loss = 0.0\n",
    "            self.flush(kw['logger'],\n",
    "                       ','.join(str(x) for x in [0, kw['epoch'], i, p, r, f1, a, current_loss]))"
   ],
   "execution_count": 31,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training setup"
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "\"\"\"\n### author: Aashis Khanal\n### sraashis@gmail.com\n### date: 9/10/2018\n\"\"\"\n\nimport os\nimport traceback\n\nimport torch\nimport torch.optim as optim\n\n\ndef run(runs):\n    for R in runs:\n        for k, folder in R['Dirs'].items():\n            os.makedirs(folder, exist_ok=True)\n        R['acc'] = ScoreAccumulator()\n        R['checkpoint_file'] = R['train_mapping_file'].split(os.sep)[1] + '.tar'\n        model = SkullNet(R['Params']['num_channels'], R['Params']['num_classes'])\n        optimizer = optim.Adam(model.parameters(), lr=R['Params']['learning_rate'])\n        if R['Params']['distribute']:\n            model = torch.nn.DataParallel(model)\n            model.float()\n            optimizer = optim.Adam(model.module.parameters(), lr=R['Params']['learning_rate'])\n\n        try:\n            trainer = NNTrainer(model=model, conf=R, optimizer=optimizer)\n            if R.get('Params').get('mode') == 'train':\n                train_loader, val_loader = SkullDataset.get_train_val_loader(R)\n                print('### Train Val Batch size:', len(train_loader.dataset), len(val_loader.dataset))\n                trainer.train(data_loader=train_loader, validation_loader=val_loader,\n                              epoch_run=trainer.epoch_ce_loss)\n\n            test_loader = SkullDataset.get_test_loader(conf=R)\n            trainer.resume_from_checkpoint(parallel_trained=R.get('Params').get('parallel_trained'))\n            \n            trainer.test(test_loader)\n        except Exception as e:\n            traceback.print_exc()\n\n    print(R['acc'].get_prfa())\n    f = open(R['Dirs']['logs'] + os.sep + 'score.txt', \"w\")\n    f.write(', '.join(str(s) for s in R['acc'].get_prfa()))\n    f.close()\n",
   "execution_count": 28,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "Params = {\n    'num_channels': 1,\n    'num_classes': 2,\n    'batch_size': 16,\n    'epochs': 1,\n    'learning_rate': 0.001,\n    'use_gpu': True,\n    'distribute': True,\n    'shuffle': True,\n    'log_frequency': 10,\n    'validation_frequency': 1,\n    'mode': 'train',\n    'parallel_trained': False,\n}\nSKDB = {\n    'Params': Params,\n    'Dirs': {\n        'train_image_dir': train_images_dir,\n        'test_image_dir': test_images_dir,\n        'logs': 'logs'},\n    'train_mapping_file': train_mapping_file,\n    'test_mapping_file': test_mapping_file,\n    'f_dyn_weights': lambda x: np.random.choice(np.arange(1, 101, 1), 2),\n    'validate_img_pth': False,\n    'load_lim': 10e10\n}",
   "execution_count": 32,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true,
    "scrolled": false,
    "_kg_hide-output": true
   },
   "cell_type": "code",
   "source": "run([SKDB])",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": "### CRITICAL!!! logs/submission.csv\" already exists.\nOverride? [Y/N]: y\nOverride? [Y/N]: \n### CRITICAL!!! logs/input-TRAIN.csv\" already exists.\n### CRITICAL!!! logs/input-VAL.csv\" already exists.\nOverride? [Y/N]: \n### Train Val Batch size: 539410 134852\nTraining...\n",
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": "/kaggle/usr/lib/img_utils/img_utils.py:102: RuntimeWarning: overflow encountered in short_scalars\n  return (arr - n) / (m - n)\n",
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": "Epochs[1/1] Batch[10/33713] loss:0.52511 pre:0.001 rec:0.001 f1:0.001 acc:0.938\nEpochs[1/1] Batch[20/33713] loss:0.26050 pre:0.001 rec:0.001 f1:0.001 acc:0.958\nEpochs[1/1] Batch[30/33713] loss:0.26987 pre:0.001 rec:0.001 f1:0.001 acc:0.979\nEpochs[1/1] Batch[40/33713] loss:0.32615 pre:0.001 rec:0.001 f1:0.001 acc:0.938\nEpochs[1/1] Batch[50/33713] loss:0.35574 pre:0.001 rec:0.001 f1:0.001 acc:0.896\nEpochs[1/1] Batch[60/33713] loss:0.23152 pre:0.001 rec:0.001 f1:0.001 acc:0.938\nEpochs[1/1] Batch[70/33713] loss:0.22240 pre:0.001 rec:0.001 f1:0.001 acc:0.917\nEpochs[1/1] Batch[80/33713] loss:0.32213 pre:0.001 rec:0.001 f1:0.001 acc:0.948\nEpochs[1/1] Batch[90/33713] loss:0.24943 pre:0.001 rec:0.001 f1:0.001 acc:0.927\nEpochs[1/1] Batch[100/33713] loss:0.32838 pre:0.001 rec:0.001 f1:0.001 acc:0.938\nEpochs[1/1] Batch[110/33713] loss:0.21007 pre:0.001 rec:0.001 f1:0.001 acc:0.938\nEpochs[1/1] Batch[120/33713] loss:0.18801 pre:0.001 rec:0.001 f1:0.001 acc:0.927\nEpochs[1/1] Batch[130/33713] loss:0.22263 pre:0.001 rec:0.001 f1:0.001 acc:0.948\nEpochs[1/1] Batch[140/33713] loss:0.38705 pre:0.001 rec:0.001 f1:0.001 acc:0.979\nEpochs[1/1] Batch[150/33713] loss:0.25552 pre:0.001 rec:0.001 f1:0.001 acc:0.979\nEpochs[1/1] Batch[160/33713] loss:0.26887 pre:0.001 rec:0.001 f1:0.001 acc:0.948\nEpochs[1/1] Batch[170/33713] loss:0.25484 pre:0.001 rec:0.001 f1:0.001 acc:0.938\n",
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": "/kaggle/usr/lib/img_utils/img_utils.py:102: RuntimeWarning: overflow encountered in short_scalars\n  return (arr - n) / (m - n)\n",
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": "Epochs[1/1] Batch[180/33713] loss:0.39891 pre:0.001 rec:0.001 f1:0.001 acc:0.948\nEpochs[1/1] Batch[190/33713] loss:0.35096 pre:0.500 rec:0.111 f1:0.182 acc:0.906\nEpochs[1/1] Batch[200/33713] loss:0.40018 pre:0.001 rec:0.001 f1:0.001 acc:0.917\nEpochs[1/1] Batch[210/33713] loss:0.29322 pre:0.001 rec:0.001 f1:0.001 acc:0.854\nEpochs[1/1] Batch[220/33713] loss:0.37435 pre:0.001 rec:0.001 f1:0.001 acc:0.979\nEpochs[1/1] Batch[230/33713] loss:0.22027 pre:0.001 rec:0.001 f1:0.001 acc:0.927\nEpochs[1/1] Batch[240/33713] loss:0.30427 pre:0.001 rec:0.001 f1:0.001 acc:0.979\nEpochs[1/1] Batch[250/33713] loss:0.34181 pre:0.250 rec:0.200 f1:0.222 acc:0.927\nEpochs[1/1] Batch[260/33713] loss:0.19343 pre:0.001 rec:0.001 f1:0.001 acc:0.938\nEpochs[1/1] Batch[270/33713] loss:0.22397 pre:0.001 rec:0.001 f1:0.001 acc:0.917\nEpochs[1/1] Batch[280/33713] loss:0.13886 pre:0.001 rec:0.001 f1:0.001 acc:0.979\nEpochs[1/1] Batch[290/33713] loss:0.29712 pre:0.001 rec:0.001 f1:0.001 acc:0.938\nEpochs[1/1] Batch[300/33713] loss:0.21140 pre:0.001 rec:0.001 f1:0.001 acc:0.927\nEpochs[1/1] Batch[310/33713] loss:0.19703 pre:0.001 rec:0.001 f1:0.001 acc:0.958\nEpochs[1/1] Batch[320/33713] loss:0.21203 pre:0.001 rec:0.001 f1:0.001 acc:0.979\nEpochs[1/1] Batch[330/33713] loss:0.13603 pre:0.001 rec:0.001 f1:0.001 acc:1.000\nEpochs[1/1] Batch[340/33713] loss:0.16769 pre:0.001 rec:0.001 f1:0.001 acc:0.958\nEpochs[1/1] Batch[350/33713] loss:0.28533 pre:0.001 rec:0.001 f1:0.001 acc:0.958\nEpochs[1/1] Batch[360/33713] loss:0.19960 pre:0.001 rec:0.001 f1:0.001 acc:0.885\nEpochs[1/1] Batch[370/33713] loss:0.32754 pre:0.001 rec:0.001 f1:0.001 acc:0.938\nEpochs[1/1] Batch[380/33713] loss:0.25039 pre:0.001 rec:0.001 f1:0.001 acc:0.958\nEpochs[1/1] Batch[390/33713] loss:0.20588 pre:0.001 rec:0.001 f1:0.001 acc:0.917\nEpochs[1/1] Batch[400/33713] loss:0.19766 pre:0.001 rec:0.001 f1:0.001 acc:0.906\nEpochs[1/1] Batch[410/33713] loss:0.33538 pre:0.001 rec:0.001 f1:0.001 acc:0.917\nEpochs[1/1] Batch[420/33713] loss:0.23479 pre:0.001 rec:0.001 f1:0.001 acc:0.958\nEpochs[1/1] Batch[430/33713] loss:0.32577 pre:0.001 rec:0.001 f1:0.001 acc:1.000\nEpochs[1/1] Batch[440/33713] loss:0.22616 pre:0.001 rec:0.001 f1:0.001 acc:0.938\nEpochs[1/1] Batch[450/33713] loss:0.18827 pre:0.001 rec:0.001 f1:0.001 acc:1.000\nEpochs[1/1] Batch[460/33713] loss:0.17772 pre:0.001 rec:0.001 f1:0.001 acc:0.927\nEpochs[1/1] Batch[470/33713] loss:0.13352 pre:0.001 rec:0.001 f1:0.001 acc:0.958\nEpochs[1/1] Batch[480/33713] loss:0.23960 pre:0.001 rec:0.001 f1:0.001 acc:0.979\nEpochs[1/1] Batch[490/33713] loss:0.12369 pre:0.001 rec:0.001 f1:0.001 acc:0.979\nEpochs[1/1] Batch[500/33713] loss:0.32677 pre:0.001 rec:0.001 f1:0.001 acc:1.000\nEpochs[1/1] Batch[510/33713] loss:0.60815 pre:0.001 rec:0.001 f1:0.001 acc:0.865\nEpochs[1/1] Batch[520/33713] loss:0.28106 pre:0.001 rec:0.001 f1:0.001 acc:0.958\nEpochs[1/1] Batch[530/33713] loss:0.21700 pre:0.001 rec:0.001 f1:0.001 acc:0.958\nEpochs[1/1] Batch[540/33713] loss:0.31033 pre:0.001 rec:0.001 f1:0.001 acc:0.979\nEpochs[1/1] Batch[550/33713] loss:0.22111 pre:0.001 rec:0.001 f1:0.001 acc:0.875\nEpochs[1/1] Batch[560/33713] loss:0.18192 pre:0.001 rec:0.001 f1:0.001 acc:0.958\nEpochs[1/1] Batch[570/33713] loss:0.27761 pre:0.001 rec:0.001 f1:0.001 acc:0.958\nEpochs[1/1] Batch[580/33713] loss:0.28541 pre:0.001 rec:0.001 f1:0.001 acc:0.979\nEpochs[1/1] Batch[590/33713] loss:0.37744 pre:0.001 rec:0.001 f1:0.001 acc:0.948\nEpochs[1/1] Batch[600/33713] loss:0.21372 pre:0.001 rec:0.001 f1:0.001 acc:0.906\nEpochs[1/1] Batch[610/33713] loss:0.22218 pre:0.001 rec:0.001 f1:0.001 acc:0.938\nEpochs[1/1] Batch[620/33713] loss:0.41967 pre:0.001 rec:0.001 f1:0.001 acc:1.000\nEpochs[1/1] Batch[630/33713] loss:0.23910 pre:0.500 rec:0.200 f1:0.286 acc:0.948\nEpochs[1/1] Batch[640/33713] loss:0.24349 pre:0.001 rec:0.001 f1:0.001 acc:0.938\nEpochs[1/1] Batch[650/33713] loss:0.24278 pre:0.001 rec:0.001 f1:0.001 acc:0.885\nEpochs[1/1] Batch[660/33713] loss:0.23969 pre:0.001 rec:0.001 f1:0.001 acc:0.875\nEpochs[1/1] Batch[670/33713] loss:0.48886 pre:0.001 rec:0.001 f1:0.001 acc:0.917\nEpochs[1/1] Batch[680/33713] loss:0.27506 pre:0.800 rec:0.333 f1:0.471 acc:0.906\nEpochs[1/1] Batch[690/33713] loss:0.23496 pre:0.429 rec:0.273 f1:0.333 acc:0.875\nEpochs[1/1] Batch[700/33713] loss:0.36865 pre:0.001 rec:0.001 f1:0.001 acc:1.000\nEpochs[1/1] Batch[710/33713] loss:0.16193 pre:0.001 rec:0.001 f1:0.001 acc:0.958\nEpochs[1/1] Batch[720/33713] loss:0.32729 pre:0.001 rec:0.001 f1:0.001 acc:0.938\nEpochs[1/1] Batch[730/33713] loss:0.30237 pre:0.001 rec:0.001 f1:0.001 acc:0.906\nEpochs[1/1] Batch[740/33713] loss:0.14520 pre:0.001 rec:0.001 f1:0.001 acc:0.979\nEpochs[1/1] Batch[750/33713] loss:0.14302 pre:0.001 rec:0.001 f1:0.001 acc:0.969\nEpochs[1/1] Batch[760/33713] loss:0.23720 pre:0.001 rec:0.001 f1:0.001 acc:0.927\nEpochs[1/1] Batch[770/33713] loss:0.25342 pre:0.001 rec:0.001 f1:0.001 acc:0.979\nEpochs[1/1] Batch[780/33713] loss:0.15129 pre:0.001 rec:0.001 f1:0.001 acc:0.958\nEpochs[1/1] Batch[790/33713] loss:0.16664 pre:0.001 rec:0.001 f1:0.001 acc:0.927\nEpochs[1/1] Batch[800/33713] loss:0.21696 pre:0.001 rec:0.001 f1:0.001 acc:0.927\nEpochs[1/1] Batch[810/33713] loss:0.41244 pre:0.001 rec:0.001 f1:0.001 acc:0.906\nEpochs[1/1] Batch[820/33713] loss:0.21948 pre:0.001 rec:0.001 f1:0.001 acc:0.938\nEpochs[1/1] Batch[830/33713] loss:0.29854 pre:0.001 rec:0.001 f1:0.001 acc:0.948\nEpochs[1/1] Batch[840/33713] loss:0.21905 pre:0.001 rec:0.001 f1:0.001 acc:0.979\nEpochs[1/1] Batch[850/33713] loss:0.23029 pre:0.001 rec:0.001 f1:0.001 acc:0.958\nEpochs[1/1] Batch[860/33713] loss:0.18847 pre:0.001 rec:0.001 f1:0.001 acc:0.979\nEpochs[1/1] Batch[870/33713] loss:0.27299 pre:0.001 rec:0.001 f1:0.001 acc:0.948\nEpochs[1/1] Batch[880/33713] loss:0.20411 pre:0.001 rec:0.001 f1:0.001 acc:0.969\nEpochs[1/1] Batch[890/33713] loss:0.35044 pre:0.001 rec:0.001 f1:0.001 acc:0.969\nEpochs[1/1] Batch[900/33713] loss:0.33857 pre:0.001 rec:0.001 f1:0.001 acc:0.896\nEpochs[1/1] Batch[910/33713] loss:0.23647 pre:0.500 rec:0.250 f1:0.333 acc:0.958\nEpochs[1/1] Batch[920/33713] loss:0.35856 pre:0.667 rec:0.167 f1:0.267 acc:0.885\nEpochs[1/1] Batch[930/33713] loss:0.35261 pre:0.001 rec:0.001 f1:0.001 acc:0.927\nEpochs[1/1] Batch[940/33713] loss:0.23521 pre:0.667 rec:0.200 f1:0.308 acc:0.906\nEpochs[1/1] Batch[950/33713] loss:0.16927 pre:0.001 rec:0.001 f1:0.001 acc:0.906\nEpochs[1/1] Batch[960/33713] loss:0.31025 pre:0.001 rec:0.001 f1:0.001 acc:1.000\nEpochs[1/1] Batch[970/33713] loss:0.33623 pre:0.001 rec:0.001 f1:0.001 acc:1.000\nEpochs[1/1] Batch[980/33713] loss:0.24660 pre:0.001 rec:0.001 f1:0.001 acc:0.979\nEpochs[1/1] Batch[990/33713] loss:0.42076 pre:1.000 rec:0.250 f1:0.400 acc:0.969\nEpochs[1/1] Batch[1000/33713] loss:0.24719 pre:1.000 rec:0.143 f1:0.250 acc:0.938\nEpochs[1/1] Batch[1010/33713] loss:0.41675 pre:1.000 rec:0.250 f1:0.400 acc:0.875\nEpochs[1/1] Batch[1020/33713] loss:0.20791 pre:0.500 rec:0.400 f1:0.444 acc:0.948\nEpochs[1/1] Batch[1030/33713] loss:0.24504 pre:0.001 rec:0.001 f1:0.001 acc:0.958\nEpochs[1/1] Batch[1040/33713] loss:0.19423 pre:0.001 rec:0.001 f1:0.001 acc:1.000\nEpochs[1/1] Batch[1050/33713] loss:0.29682 pre:0.001 rec:0.001 f1:0.001 acc:0.906\nEpochs[1/1] Batch[1060/33713] loss:0.21070 pre:0.001 rec:0.001 f1:0.001 acc:0.927\nEpochs[1/1] Batch[1070/33713] loss:0.22600 pre:0.001 rec:0.001 f1:0.001 acc:0.979\nEpochs[1/1] Batch[1080/33713] loss:0.26234 pre:0.001 rec:0.001 f1:0.001 acc:0.917\nEpochs[1/1] Batch[1090/33713] loss:0.24804 pre:0.001 rec:0.001 f1:0.001 acc:0.979\nEpochs[1/1] Batch[1100/33713] loss:0.25539 pre:0.001 rec:0.001 f1:0.001 acc:0.906\nEpochs[1/1] Batch[1110/33713] loss:0.18003 pre:0.001 rec:0.001 f1:0.001 acc:1.000\nEpochs[1/1] Batch[1120/33713] loss:0.26185 pre:0.001 rec:0.001 f1:0.001 acc:0.927\nEpochs[1/1] Batch[1130/33713] loss:0.19818 pre:0.001 rec:0.001 f1:0.001 acc:0.927\nEpochs[1/1] Batch[1140/33713] loss:0.17350 pre:0.001 rec:0.001 f1:0.001 acc:0.927\nEpochs[1/1] Batch[1150/33713] loss:0.20431 pre:0.001 rec:0.001 f1:0.001 acc:0.917\nEpochs[1/1] Batch[1160/33713] loss:0.16562 pre:0.001 rec:0.001 f1:0.001 acc:0.885\nEpochs[1/1] Batch[1170/33713] loss:0.32583 pre:0.001 rec:0.001 f1:0.001 acc:0.896\nEpochs[1/1] Batch[1180/33713] loss:0.15790 pre:0.001 rec:0.001 f1:0.001 acc:0.979\n",
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": "Epochs[1/1] Batch[1190/33713] loss:0.15501 pre:0.001 rec:0.001 f1:0.001 acc:0.865\nEpochs[1/1] Batch[1200/33713] loss:0.13682 pre:0.001 rec:0.001 f1:0.001 acc:0.917\nEpochs[1/1] Batch[1210/33713] loss:0.30174 pre:0.001 rec:0.001 f1:0.001 acc:0.927\nEpochs[1/1] Batch[1220/33713] loss:0.31443 pre:0.001 rec:0.001 f1:0.001 acc:0.927\nEpochs[1/1] Batch[1230/33713] loss:0.22864 pre:0.001 rec:0.001 f1:0.001 acc:0.958\nEpochs[1/1] Batch[1240/33713] loss:0.30885 pre:0.001 rec:0.001 f1:0.001 acc:0.917\nEpochs[1/1] Batch[1250/33713] loss:0.17251 pre:0.001 rec:0.001 f1:0.001 acc:0.938\nEpochs[1/1] Batch[1260/33713] loss:0.30929 pre:0.001 rec:0.001 f1:0.001 acc:0.896\nEpochs[1/1] Batch[1270/33713] loss:0.54710 pre:0.001 rec:0.001 f1:0.001 acc:0.979\nEpochs[1/1] Batch[1280/33713] loss:0.26690 pre:0.556 rec:0.385 f1:0.455 acc:0.875\nEpochs[1/1] Batch[1290/33713] loss:0.24400 pre:0.200 rec:0.125 f1:0.154 acc:0.885\nEpochs[1/1] Batch[1300/33713] loss:0.22284 pre:0.001 rec:0.001 f1:0.001 acc:0.958\nEpochs[1/1] Batch[1310/33713] loss:0.23245 pre:0.001 rec:0.001 f1:0.001 acc:0.958\nEpochs[1/1] Batch[1320/33713] loss:0.30689 pre:0.001 rec:0.001 f1:0.001 acc:0.948\nEpochs[1/1] Batch[1330/33713] loss:0.25119 pre:0.001 rec:0.001 f1:0.001 acc:0.927\nEpochs[1/1] Batch[1340/33713] loss:0.39098 pre:1.000 rec:0.500 f1:0.667 acc:0.990\nEpochs[1/1] Batch[1350/33713] loss:0.23202 pre:0.001 rec:0.001 f1:0.001 acc:0.896\nEpochs[1/1] Batch[1360/33713] loss:0.18482 pre:0.001 rec:0.001 f1:0.001 acc:0.917\nEpochs[1/1] Batch[1370/33713] loss:0.20074 pre:0.001 rec:0.001 f1:0.001 acc:0.927\nEpochs[1/1] Batch[1380/33713] loss:0.21941 pre:0.001 rec:0.001 f1:0.001 acc:1.000\nEpochs[1/1] Batch[1390/33713] loss:0.37307 pre:0.001 rec:0.001 f1:0.001 acc:0.938\nEpochs[1/1] Batch[1400/33713] loss:0.21449 pre:0.001 rec:0.001 f1:0.001 acc:0.896\nEpochs[1/1] Batch[1410/33713] loss:0.24122 pre:0.001 rec:0.001 f1:0.001 acc:0.896\nEpochs[1/1] Batch[1420/33713] loss:0.19687 pre:0.001 rec:0.001 f1:0.001 acc:0.979\nEpochs[1/1] Batch[1430/33713] loss:0.35415 pre:0.001 rec:0.001 f1:0.001 acc:0.969\nEpochs[1/1] Batch[1440/33713] loss:0.24933 pre:0.001 rec:0.001 f1:0.001 acc:0.979\nEpochs[1/1] Batch[1450/33713] loss:0.21869 pre:0.001 rec:0.001 f1:0.001 acc:0.948\nEpochs[1/1] Batch[1460/33713] loss:0.21946 pre:0.001 rec:0.001 f1:0.001 acc:0.958\nEpochs[1/1] Batch[1470/33713] loss:0.26050 pre:0.001 rec:0.001 f1:0.001 acc:0.979\nEpochs[1/1] Batch[1480/33713] loss:0.24779 pre:1.000 rec:0.167 f1:0.286 acc:0.948\nEpochs[1/1] Batch[1490/33713] loss:0.17830 pre:0.001 rec:0.001 f1:0.001 acc:0.938\nEpochs[1/1] Batch[1500/33713] loss:0.19606 pre:0.001 rec:0.001 f1:0.001 acc:0.958\nEpochs[1/1] Batch[1510/33713] loss:0.43246 pre:0.001 rec:0.001 f1:0.001 acc:0.948\nEpochs[1/1] Batch[1520/33713] loss:0.41615 pre:0.001 rec:0.001 f1:0.001 acc:0.896\nEpochs[1/1] Batch[1530/33713] loss:0.31740 pre:0.001 rec:0.001 f1:0.001 acc:0.948\nEpochs[1/1] Batch[1540/33713] loss:0.33442 pre:0.333 rec:0.111 f1:0.167 acc:0.896\nEpochs[1/1] Batch[1550/33713] loss:0.18544 pre:0.333 rec:0.083 f1:0.133 acc:0.865\nEpochs[1/1] Batch[1560/33713] loss:0.25323 pre:1.000 rec:0.167 f1:0.286 acc:0.948\nEpochs[1/1] Batch[1570/33713] loss:0.24120 pre:0.001 rec:0.001 f1:0.001 acc:0.938\nEpochs[1/1] Batch[1580/33713] loss:0.35913 pre:1.000 rec:0.250 f1:0.400 acc:0.938\n",
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": "/kaggle/usr/lib/img_utils/img_utils.py:102: RuntimeWarning: invalid value encountered in true_divide\n  return (arr - n) / (m - n)\n",
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": "Epochs[1/1] Batch[1590/33713] loss:0.27760 pre:0.500 rec:0.286 f1:0.364 acc:0.927\nEpochs[1/1] Batch[1600/33713] loss:0.26085 pre:0.500 rec:0.500 f1:0.500 acc:0.979\nEpochs[1/1] Batch[1610/33713] loss:0.28235 pre:0.001 rec:0.001 f1:0.001 acc:0.979\nEpochs[1/1] Batch[1620/33713] loss:0.20165 pre:0.001 rec:0.001 f1:0.001 acc:0.948\nEpochs[1/1] Batch[1630/33713] loss:0.30816 pre:0.001 rec:0.001 f1:0.001 acc:1.000\nEpochs[1/1] Batch[1640/33713] loss:0.15731 pre:0.001 rec:0.001 f1:0.001 acc:0.958\n",
     "name": "stdout"
    }
   ]
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "version": "3.6.4",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python",
   "mimetype": "text/x-python"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}